{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/ubuntu/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/ubuntu/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/ubuntu/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/ubuntu/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/ubuntu/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/ubuntu/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import string\n",
    "try:\n",
    "    maketrans = ''.maketrans\n",
    "except AttributeError:\n",
    "    # fallback for Python 2\n",
    "    from string import maketrans\n",
    "\n",
    "import pandas as pd\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout,LSTM, Embedding, CuDNNLSTM, Bidirectional,SpatialDropout1D\n",
    "from keras.callbacks.callbacks import EarlyStopping\n",
    "from keras import initializers,regularizers,constraints\n",
    "from keras.layers import Layer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.backend import sigmoid\n",
    "import keras.backend as K\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import cross_val_score "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocessing(document):\n",
    "    document[\"Question\"] = document[\"Question\"].apply(lambda x: x.lower() \\\n",
    "    .translate(maketrans(\"\",\"\", string.punctuation)) \\\n",
    "    .strip()) \n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    stemmer= PorterStemmer()\n",
    "    document[\"Question\"] = document[\"Question\"].apply(lambda x: remove_stopwords(x,stop_words,stemmer))\n",
    "    display(document)\n",
    "    return document\n",
    "\n",
    "def remove_stopwords(sentense,stop_words,stemmer):\n",
    "    tokens = word_tokenize(sentense)\n",
    "    result = [stemmer.stem(i) for i in tokens if not i in stop_words]\n",
    "    return ' '.join(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Swish (Self-defined activation function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def swish(x, beta = 1):\n",
    "    return (x * sigmoid(beta * x))\n",
    "\n",
    "from keras.utils.generic_utils import get_custom_objects\n",
    "from keras.layers import Activation\n",
    "get_custom_objects().update({'swish': Activation(swish)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot_product(x, kernel):\n",
    "    \"\"\"\n",
    "    Wrapper for dot product operation, in order to be compatible with both\n",
    "    Theano and Tensorflow\n",
    "    Args:\n",
    "        x (): input\n",
    "        kernel (): weights\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    if K.backend() == 'tensorflow':\n",
    "        return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)\n",
    "    else:\n",
    "        return K.dot(x, kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionWithContext(Layer):\n",
    "    \"\"\"\n",
    "    Attention operation, with a context/query vector, for temporal data.\n",
    "    Supports Masking.\n",
    "    Follows the work of Yang et al. [https://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf]\n",
    "    \"Hierarchical Attention Networks for Document Classification\"\n",
    "    by using a context vector to assist the attention\n",
    "    # Input shape\n",
    "        3D tensor with shape: `(samples, steps, features)`.\n",
    "    # Output shape\n",
    "        2D tensor with shape: `(samples, features)`.\n",
    "    How to use:\n",
    "    Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
    "    The dimensions are inferred based on the output shape of the RNN.\n",
    "    Note: The layer has been tested with Keras 2.0.6\n",
    "    Example:\n",
    "        model.add(LSTM(64, return_sequences=True))\n",
    "        model.add(AttentionWithContext())\n",
    "        # next add a Dense layer (for classification/regression) or whatever...\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 W_regularizer=None, u_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, u_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.u_regularizer = regularizers.get(u_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.u_constraint = constraints.get(u_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        super(AttentionWithContext, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight(shape = (input_shape[-1], input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight(shape = (input_shape[-1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "\n",
    "        self.u = self.add_weight(shape = (input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_u'.format(self.name),\n",
    "                                 regularizer=self.u_regularizer,\n",
    "                                 constraint=self.u_constraint)\n",
    "\n",
    "        super(AttentionWithContext, self).build(input_shape)\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        uit = dot_product(x, self.W)\n",
    "\n",
    "        if self.bias:\n",
    "            uit += self.b\n",
    "\n",
    "        uit = K.tanh(uit)\n",
    "        ait = dot_product(uit, self.u)\n",
    "\n",
    "        a = K.exp(ait)\n",
    "\n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        # in some cases especially in the early stages of training the sum may be almost zero\n",
    "        # and this results in NaN's. A workaround is to add a very small positive number ε to the sum.\n",
    "        # a /= K.cast(K.sum(a, axis=1, keepdims=True), K.floatx())\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0], input_shape[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "The following code produce two BiDirectional RNN models. They both contain an embedding layer and LSTM (Long Short-Term Memory) layer. RNNwithAttention refer to the model with an extra layer of attention. While RNN does not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>follow segment code part program use insert so...</td>\n",
       "      <td>Counting Lists, Permutations, and Subsets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>five school go send basebal team tournament te...</td>\n",
       "      <td>Counting Lists, Permutations, and Subsets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mani way draw first card second card deck 52 card</td>\n",
       "      <td>Counting Lists, Permutations, and Subsets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mani way draw two card deck 52 card</td>\n",
       "      <td>Counting Lists, Permutations, and Subsets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mani way draw first second third card deck 52 ...</td>\n",
       "      <td>Counting Lists, Permutations, and Subsets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>mani way tenperson club select presid secretar...</td>\n",
       "      <td>Counting Lists, Permutations, and Subsets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>mani way tenperson club select twoperson execu...</td>\n",
       "      <td>Counting Lists, Permutations, and Subsets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>mani way tenperson club select presid twoperso...</td>\n",
       "      <td>Counting Lists, Permutations, and Subsets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>element set n nelement set mani order pair fir...</td>\n",
       "      <td>Counting Lists, Permutations, and Subsets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>local ice cream shop sell ten differ flavor ic...</td>\n",
       "      <td>Counting Lists, Permutations, and Subsets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>suppos decid disagre mother problem 11—the ord...</td>\n",
       "      <td>Counting Lists, Permutations, and Subsets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>suppos day 1 receiv one penni 1 day receiv twi...</td>\n",
       "      <td>Counting Lists, Permutations, and Subsets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>pile high deli offer simpl sandwich consist ch...</td>\n",
       "      <td>Counting Lists, Permutations, and Subsets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>15 see unnecessari step pseudocod exercis 113 ...</td>\n",
       "      <td>Counting Lists, Permutations, and Subsets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>assum k n mani way pass k distinct piec fruit ...</td>\n",
       "      <td>Counting Lists, Permutations, and Subsets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>assum k n mani way pass k ident piec fruit n c...</td>\n",
       "      <td>Counting Lists, Permutations, and Subsets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>mani base ten number five digit mani fivedigit...</td>\n",
       "      <td>Counting Lists, Permutations, and Subsets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>suppos organ panel discuss allow alcohol campu...</td>\n",
       "      <td>Counting Lists, Permutations, and Subsets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>problem student work relationship kelement per...</td>\n",
       "      <td>Counting Lists, Permutations, and Subsets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>mani way class 20 student choos group 3 studen...</td>\n",
       "      <td>Counting Lists, Permutations, and Subsets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>suppos choos particip panel discuss allow alco...</td>\n",
       "      <td>Counting Lists, Permutations, and Subsets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>suppos organ panel discuss allow alcohol campu...</td>\n",
       "      <td>Counting Lists, Permutations, and Subsets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>local ice cream shop may get sunda two scoop i...</td>\n",
       "      <td>Counting Lists, Permutations, and Subsets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>local ice cream shop may get threeway sunda th...</td>\n",
       "      <td>Counting Lists, Permutations, and Subsets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>tenni club 2n member want pair member two sing...</td>\n",
       "      <td>Counting Lists, Permutations, and Subsets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>basketbal team 12 player howev 5 player play g...</td>\n",
       "      <td>Counting Lists, Permutations, and Subsets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>explain function nelement set nelement set one...</td>\n",
       "      <td>Counting Lists, Permutations, and Subsets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>function g call invers function f domain g ran...</td>\n",
       "      <td>Counting Lists, Permutations, and Subsets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>find row pascal triangl correspond n 8</td>\n",
       "      <td>Counting Lists, Permutations, and Subsets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>find follow x 15f b x y5 c x 25 x — 15</td>\n",
       "      <td>Counting Lists, Permutations, and Subsets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>recurs tree work even problem break geometr wo...</td>\n",
       "      <td>The Master Theorem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>case problem 14 big bound found big 0 hound</td>\n",
       "      <td>The Master Theorem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>sn asn — 1 gn gn cn 1 c fast sn grow big 0 term</td>\n",
       "      <td>The Master Theorem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>sn asn — 1 gn gn cn 0 c fast sn grow big 0 lcrm</td>\n",
       "      <td>The Master Theorem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>suppos given recurr form tn atnb gn t1 0 gn 0 ...</td>\n",
       "      <td>The Master Theorem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>use master theorem give big bound solut follow...</td>\n",
       "      <td>The Master Theorem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>give big bound solut recurr tn3tn2n312 n1 n 1</td>\n",
       "      <td>The Master Theorem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>extend proof preliminari version master theore...</td>\n",
       "      <td>The Master Theorem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>prove corollari 48 show x z greater 1 x logyz ...</td>\n",
       "      <td>The Master Theorem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>show real number x 1 one one valu x given recurr</td>\n",
       "      <td>The Master Theorem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>suppos c real number greater solut tn recurr t...</td>\n",
       "      <td>The Master Theorem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>prove induct tn 4tn2 n2 tn n2 log n assum n po...</td>\n",
       "      <td>The Master Theorem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>show induct solut recurr form tn 2t clog3 n n ...</td>\n",
       "      <td>The Master Theorem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>happen replac 2 problem 3 4 still get big uppe...</td>\n",
       "      <td>The Master Theorem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>big upper bound problem 3 actual big 0 bound</td>\n",
       "      <td>The Master Theorem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>conclus problem 2 hold recurr tn 4t n2 n2 requ...</td>\n",
       "      <td>The Master Theorem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>find best big upper bound solut recurr 4tn2 n ...</td>\n",
       "      <td>The Master Theorem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>big upper bound problem 7 actual big 0 bound</td>\n",
       "      <td>The Master Theorem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>big upper bound problem 9 actual big 0 bound</td>\n",
       "      <td>The Master Theorem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>give exampl differ text function f bx f x give...</td>\n",
       "      <td>The Master Theorem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>text function f bx x 12 give best big upper bo...</td>\n",
       "      <td>The Master Theorem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>find best big bound tn satisfi recurr tn tn4 t...</td>\n",
       "      <td>The Master Theorem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>magicmiddl algorithm suppos broke data n7 set ...</td>\n",
       "      <td>The Master Theorem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>draw recurs tree big bound solut recurr use re...</td>\n",
       "      <td>The Master Theorem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>suppos given b nonneg real number b 1 c nonneg...</td>\n",
       "      <td>The Master Theorem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>find big 0 bound best know get solut recurr tn...</td>\n",
       "      <td>The Master Theorem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>find big 0 bound solut recurr tn4 t3n4 dn tn t...</td>\n",
       "      <td>The Master Theorem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>magicmiddl algorithm suppos broke data n3 set ...</td>\n",
       "      <td>The Master Theorem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>find big upper bound best know get solut recur...</td>\n",
       "      <td>The Master Theorem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>note chosen median nelement set element posit ...</td>\n",
       "      <td>The Master Theorem</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>243 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Question  \\\n",
       "0    follow segment code part program use insert so...   \n",
       "1    five school go send basebal team tournament te...   \n",
       "2    mani way draw first card second card deck 52 card   \n",
       "3                  mani way draw two card deck 52 card   \n",
       "4    mani way draw first second third card deck 52 ...   \n",
       "5    mani way tenperson club select presid secretar...   \n",
       "6    mani way tenperson club select twoperson execu...   \n",
       "7    mani way tenperson club select presid twoperso...   \n",
       "8    element set n nelement set mani order pair fir...   \n",
       "9    local ice cream shop sell ten differ flavor ic...   \n",
       "10   suppos decid disagre mother problem 11—the ord...   \n",
       "11   suppos day 1 receiv one penni 1 day receiv twi...   \n",
       "12   pile high deli offer simpl sandwich consist ch...   \n",
       "13   15 see unnecessari step pseudocod exercis 113 ...   \n",
       "14   assum k n mani way pass k distinct piec fruit ...   \n",
       "15   assum k n mani way pass k ident piec fruit n c...   \n",
       "16   mani base ten number five digit mani fivedigit...   \n",
       "17   suppos organ panel discuss allow alcohol campu...   \n",
       "18   problem student work relationship kelement per...   \n",
       "19   mani way class 20 student choos group 3 studen...   \n",
       "20   suppos choos particip panel discuss allow alco...   \n",
       "21   suppos organ panel discuss allow alcohol campu...   \n",
       "22   local ice cream shop may get sunda two scoop i...   \n",
       "23   local ice cream shop may get threeway sunda th...   \n",
       "24   tenni club 2n member want pair member two sing...   \n",
       "25   basketbal team 12 player howev 5 player play g...   \n",
       "26   explain function nelement set nelement set one...   \n",
       "27   function g call invers function f domain g ran...   \n",
       "28              find row pascal triangl correspond n 8   \n",
       "29              find follow x 15f b x y5 c x 25 x — 15   \n",
       "..                                                 ...   \n",
       "213  recurs tree work even problem break geometr wo...   \n",
       "214        case problem 14 big bound found big 0 hound   \n",
       "215    sn asn — 1 gn gn cn 1 c fast sn grow big 0 term   \n",
       "216    sn asn — 1 gn gn cn 0 c fast sn grow big 0 lcrm   \n",
       "217  suppos given recurr form tn atnb gn t1 0 gn 0 ...   \n",
       "218  use master theorem give big bound solut follow...   \n",
       "219      give big bound solut recurr tn3tn2n312 n1 n 1   \n",
       "220  extend proof preliminari version master theore...   \n",
       "221  prove corollari 48 show x z greater 1 x logyz ...   \n",
       "222   show real number x 1 one one valu x given recurr   \n",
       "223  suppos c real number greater solut tn recurr t...   \n",
       "224  prove induct tn 4tn2 n2 tn n2 log n assum n po...   \n",
       "225  show induct solut recurr form tn 2t clog3 n n ...   \n",
       "226  happen replac 2 problem 3 4 still get big uppe...   \n",
       "227       big upper bound problem 3 actual big 0 bound   \n",
       "228  conclus problem 2 hold recurr tn 4t n2 n2 requ...   \n",
       "229  find best big upper bound solut recurr 4tn2 n ...   \n",
       "230       big upper bound problem 7 actual big 0 bound   \n",
       "231       big upper bound problem 9 actual big 0 bound   \n",
       "232  give exampl differ text function f bx f x give...   \n",
       "233  text function f bx x 12 give best big upper bo...   \n",
       "234  find best big bound tn satisfi recurr tn tn4 t...   \n",
       "235  magicmiddl algorithm suppos broke data n7 set ...   \n",
       "236  draw recurs tree big bound solut recurr use re...   \n",
       "237  suppos given b nonneg real number b 1 c nonneg...   \n",
       "238  find big 0 bound best know get solut recurr tn...   \n",
       "239  find big 0 bound solut recurr tn4 t3n4 dn tn t...   \n",
       "240  magicmiddl algorithm suppos broke data n3 set ...   \n",
       "241  find big upper bound best know get solut recur...   \n",
       "242  note chosen median nelement set element posit ...   \n",
       "\n",
       "                                         Label  \n",
       "0    Counting Lists, Permutations, and Subsets  \n",
       "1    Counting Lists, Permutations, and Subsets  \n",
       "2    Counting Lists, Permutations, and Subsets  \n",
       "3    Counting Lists, Permutations, and Subsets  \n",
       "4    Counting Lists, Permutations, and Subsets  \n",
       "5    Counting Lists, Permutations, and Subsets  \n",
       "6    Counting Lists, Permutations, and Subsets  \n",
       "7    Counting Lists, Permutations, and Subsets  \n",
       "8    Counting Lists, Permutations, and Subsets  \n",
       "9    Counting Lists, Permutations, and Subsets  \n",
       "10   Counting Lists, Permutations, and Subsets  \n",
       "11   Counting Lists, Permutations, and Subsets  \n",
       "12   Counting Lists, Permutations, and Subsets  \n",
       "13   Counting Lists, Permutations, and Subsets  \n",
       "14   Counting Lists, Permutations, and Subsets  \n",
       "15   Counting Lists, Permutations, and Subsets  \n",
       "16   Counting Lists, Permutations, and Subsets  \n",
       "17   Counting Lists, Permutations, and Subsets  \n",
       "18   Counting Lists, Permutations, and Subsets  \n",
       "19   Counting Lists, Permutations, and Subsets  \n",
       "20   Counting Lists, Permutations, and Subsets  \n",
       "21   Counting Lists, Permutations, and Subsets  \n",
       "22   Counting Lists, Permutations, and Subsets  \n",
       "23   Counting Lists, Permutations, and Subsets  \n",
       "24   Counting Lists, Permutations, and Subsets  \n",
       "25   Counting Lists, Permutations, and Subsets  \n",
       "26   Counting Lists, Permutations, and Subsets  \n",
       "27   Counting Lists, Permutations, and Subsets  \n",
       "28   Counting Lists, Permutations, and Subsets  \n",
       "29   Counting Lists, Permutations, and Subsets  \n",
       "..                                         ...  \n",
       "213                         The Master Theorem  \n",
       "214                         The Master Theorem  \n",
       "215                         The Master Theorem  \n",
       "216                         The Master Theorem  \n",
       "217                         The Master Theorem  \n",
       "218                         The Master Theorem  \n",
       "219                         The Master Theorem  \n",
       "220                         The Master Theorem  \n",
       "221                         The Master Theorem  \n",
       "222                         The Master Theorem  \n",
       "223                         The Master Theorem  \n",
       "224                         The Master Theorem  \n",
       "225                         The Master Theorem  \n",
       "226                         The Master Theorem  \n",
       "227                         The Master Theorem  \n",
       "228                         The Master Theorem  \n",
       "229                         The Master Theorem  \n",
       "230                         The Master Theorem  \n",
       "231                         The Master Theorem  \n",
       "232                         The Master Theorem  \n",
       "233                         The Master Theorem  \n",
       "234                         The Master Theorem  \n",
       "235                         The Master Theorem  \n",
       "236                         The Master Theorem  \n",
       "237                         The Master Theorem  \n",
       "238                         The Master Theorem  \n",
       "239                         The Master Theorem  \n",
       "240                         The Master Theorem  \n",
       "241                         The Master Theorem  \n",
       "242                         The Master Theorem  \n",
       "\n",
       "[243 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n': 1, '1': 2, 'x': 3, 'number': 4, 'mani': 5, 'b': 6, 'integ': 7, '—': 8, 'use': 9, '2': 10, 'set': 11, 'way': 12, 'tn': 13, 'k': 14, 'recurr': 15, 'statement': 16, '“': 17, 'solut': 18, 'prove': 19, 'mod': 20, '”': 21, 'one': 22, 'c': 23, '0': 24, 'e': 25, 'p': 26, 'differ': 27, 'explain': 28, 'find': 29, 'prime': 30, 'show': 31, 'v': 32, 'two': 33, '4': 34, 'big': 35, 'assum': 36, 'follow': 37, 'element': 38, '3': 39, 'equival': 40, 'comput': 41, 'function': 42, 'power': 43, 'give': 44, '5': 45, 'bound': 46, 'choos': 47, 'multipl': 48, '’': 49, 'problem': 50, 'theorem': 51, 'posit': 52, 'equat': 53, 'suppos': 54, 'three': 55, 'write': 56, 'list': 57, 'f': 58, 'messag': 59, 'know': 60, 'equal': 61, 'invers': 62, 'relat': 63, 'z': 64, 'recurs': 65, 'draw': 66, 'solv': 67, '•': 68, 'odd': 69, 'j': 70, 'make': 71, 'may': 72, 'get': 73, 'digit': 74, 'proof': 75, 'true': 76, 'induct': 77, 'tree': 78, 'must': 79, 'product': 80, 'form': 81, 'sn': 82, 'r': 83, 'say': 84, 'principl': 85, 'real': 86, 'case': 87, 'stand': 88, 'formula': 89, 'answer': 90, 'tabl': 91, 'relationship': 92, '10': 93, '8': 94, 'peopl': 95, 'valu': 96, '9': 97, 'would': 98, 'algorithm': 99, 'q': 100, 'part': 101, 'member': 102, 'given': 103, 'everi': 104, 'place': 105, 'checker': 106, 'distribut': 107, 'gcd': 108, 'first': 109, 'possibl': 110, 'ident': 111, 'least': 112, 'group': 113, 'determin': 114, 'defin': 115, 'let': 116, 'common': 117, '7': 118, 'zn': 119, 'factor': 120, 'exampl': 121, 'even': 122, 'n2': 123, 'nelement': 124, 'order': 125, 'sum': 126, 'student': 127, 'class': 128, '11': 129, 'rel': 130, 'greater': 131, 'diagram': 132, 'euclid': 133, 'tn4': 134, 'card': 135, 'cream': 136, 'either': 137, 'step': 138, 'pass': 139, 'four': 140, 'sit': 141, 'want': 142, 'x2': 143, 'type': 144, 'nonneg': 145, '6': 146, 'bar': 147, 'extend': 148, 'express': 149, 'signatur': 150, 'fals': 151, 'px': 152, 't1': 153, 'gn': 154, 'upper': 155, 'time': 156, 'consid': 157, 'rule': 158, 'piec': 159, 'threeelement': 160, 'work': 161, 'question': 162, 'base': 163, 'allow': 164, 'administr': 165, 'subset': 166, 'addit': 167, '12': 168, 'red': 169, 'think': 170, 'repres': 171, 'symmetr': 172, 'candi': 173, '−': 174, 'law': 175, 'encrypt': 176, '23': 177, 'bob': 178, 'key': 179, 'exist': 180, 'univers': 181, 'quantifi': 182, 't0': 183, 'exact': 184, 'could': 185, 'send': 186, 'play': 187, 'club': 188, 'among': 189, 'pair': 190, 'ice': 191, 'ten': 192, 'scoop': 193, 'receiv': 194, 'pseudocod': 195, 'distinct': 196, 'fruit': 197, 'children': 198, 'mathemat': 199, 'arrang': 200, 'chosen': 201, 'also': 202, 'g': 203, 'call': 204, 'row': 205, 'label': 206, 'appl': 207, 'multiset': 208, 'squar': 209, 'letter': 210, 'reflex': 211, 'transit': 212, 'appropri': 213, '∈': 214, 'whether': 215, 'result': 216, 'run': 217, 'cn': 218, 'mn': 219, 'variabl': 220, 'vx': 221, '8tn2': 222, 'best': 223, 'program': 224, 'item': 225, 'five': 226, 'team': 227, 'presid': 228, 'committe': 229, 'flavor': 230, 'top': 231, 'day': 232, 'exercis': 233, 'child': 234, 'discuss': 235, 'need': 236, 'kelement': 237, 'permut': 238, 'actual': 239, 'take': 240, 'binomi': 241, 'x1': 242, '«': 243, 'line': 244, 'book': 245, 'divid': 246, 'black': 247, 'block': 248, '•••': 249, 'appli': 250, 'properti': 251, 'mean': 252, 'nn': 253, 'replac': 254, '13': 255, 'uniqu': 256, 'state': 257, 'divisor': 258, 'alic': 259, 'smallest': 260, 'word': 261, 'rsa': 262, 'byte': 263, 'sens': 264, 'expon': 265, 'document': 266, '—s': 267, 'alway': 268, 'definit': 269, '3z': 270, 'qx': 271, 'construct': 272, '48': 273, 'sort': 274, 'ask': 275, 'second': 276, '52': 277, 'cone': 278, 'mother': 279, '20': 280, 'onetoon': 281, 'howev': 282, 'consecut': 283, 'togeth': 284, 'correspond': 285, 'around': 286, 'sunda': 287, 'section': 288, '2n': 289, 'player': 290, 'forward': 291, 'center': 292, 'coeffici': 293, 'multinomi': 294, 'disjoint': 295, '12element': 296, 'nomin': 297, 'offic': 298, 'correct': 299, 'partit': 300, 'satisfi': 301, 'sibl': 302, 'encod': 303, '16': 304, 'without': 305, 'multipli': 306, 'straightforward': 307, 'z7': 308, 'y2': 309, 'final': 310, 'wiretapp': 311, 'size': 312, '•11': 313, '29': 314, 'might': 315, 'side': 316, 'congruenc': 317, 'modulo': 318, 'decrypt': 319, 'impli': 320, '33': 321, 'column': 322, 'sign': 323, 'tri': 324, 'truth': 325, 'corollari': 326, 'english': 327, 'symbol': 328, 'greatest': 329, 'explicitli': 330, 'u2': 331, '3x': 332, 'n3': 333, 'bx': 334, 'year': 335, 'fish': 336, 'initi': 337, 'condit': 338, 'guess': 339, '2tn': 340, '3tn3': 341, 'requir': 342, 'exactli': 343, 'game': 344, 'deck': 345, 'tenperson': 346, 'select': 347, 'local': 348, 'shop': 349, 'goe': 350, 'matter': 351, 'penni': 352, 'choic': 353, 'kind': 354, '15': 355, 'onto': 356, 'us': 357, 'finit': 358, 'panel': 359, 'alcohol': 360, 'campu': 361, 'altern': 362, '34': 363, 'guard': 364, 'rang': 365, 'fx': 366, 'care': 367, 'analyz': 368, 'seat': 369, 'els': 370, 'count': 371, 'particular': 372, 'interpret': 373, 'involv': 374, 'detail': 375, 'notat': 376, 'empti': 377, 'term': 378, 'note': 379, 'sx': 380, 'sure': 381, '∪': 382, '∩': 383, '14': 384, 'caesar': 385, 'cipher': 386, 'shift': 387, 'convert': 388, 'seem': 389, 'less': 390, 'though': 391, 'y1': 392, 'y3': 393, 'lemma': 394, '•n': 395, '133': 396, '277': 397, 'divis': 398, 'implement': 399, 'anoth': 400, 'help': 401, 'effici': 402, 'gigabyt': 403, 'terabyt': 404, 'store': 405, '10120': 406, 'spreadsheet': 407, 'hint': 408, 'put': 409, 'u': 410, '—p': 411, '®': 412, 'simplifi': 413, 'explicit': 414, 'z2': 415, 'u1': 416, 'upx': 417, '3y': 418, 'contradict': 419, 'n2n': 420, 'strong': 421, 'pn': 422, 'm1': 423, 'lake': 424, 'rtn': 425, '4tn2': 426, '3tn2': 427, 'log': 428, 'happen': 429, 'text': 430, 't3n4': 431, 'median': 432, 'segment': 433, 'code': 434, 'aj': 435, 'aj1': 436, 'comparison': 437, 'go': 438, 'twoperson': 439, 'execut': 440, 'advisori': 441, 'board': 442, 'twoscoop': 443, 'vanilla': 444, 'chocol': 445, 'justifi': 446, 'pile': 447, 'high': 448, 'simpl': 449, 'sandwich': 450, 'consist': 451, 'bread': 452, 'meat': 453, 'chees': 454, 'see': 455, 'restrict': 456, 'twoelement': 457, 'thu': 458, 'organ': 459, 'behind': 460, 'particip': 461, 'none': 462, 'whip': 463, 'nut': 464, 'cherri': 465, 'accord': 466, 'dish': 467, 'specifi': 468, 'coach': 469, 'domain': 470, 'biject': 471, 'functioni': 472, 'paint': 473, 'polynomi': 474, 'coordin': 475, 'system': 476, 'path': 477, 'instead': 478, '3element': 479, 'well': 480, 'round': 481, 'everyon': 482, 'quotient': 483, 'left': 484, 'explan': 485, 'xn': 486, 'xi': 487, 'famou': 488, 'difficult': 489, 'famili': 490, 's1': 491, 'rotat': 492, 'degre': 493, 'corner': 494, 'creat': 495, 'footnot': 496, 'brother': 497, 'charl': 498, 'short': 499, 'person': 500, 'nobodi': 501, 'h': 502, 'na': 503, 'ps': 504, 'pp': 505, '∅': 506, 'right': 507, 'ad': 508, 'knowledg': 509, 'hand': 510, 'integr': 511, '•12': 512, '•nx': 513, 'z5': 514, 'z9': 515, 'recal': 516, 'zp': 517, 'x3': 518, 'arbitrari': 519, 'adversari': 520, 'revers': 521, 'add': 522, 'continu': 523, 'separ': 524, 'guarante': 525, '2m': 526, 'nonzero': 527, 'gener': 528, '210': 529, 'jq': 530, 'gcdr': 531, 'phone': 532, 'secret': 533, 'bq': 534, 'aq': 535, 'abq': 536, 'worri': 537, 'scheme': 538, 'qab': 539, 'stumbl': 540, 'someth': 541, 'version': 542, '103': 543, 'z35': 544, 'long': 545, 'increas': 546, 'whose': 547, 'conclud': 548, '„': 549, 'counterexampl': 550, 'strike': 551, 'similar': 552, 'observ': 553, '43': 554, '143': 555, '199': 556, '1111': 557, 'z1176': 558, 'z1247': 559, 'calcul': 560, 'z73': 561, 'p2': 562, 'zp2': 563, 'xp': 564, 'paragraph': 565, 'preced': 566, '223': 567, 'smaller': 568, 'xn—1': 569, 'a8': 570, 'a32': 571, 'billion': 572, 'trillion': 573, 'bit': 574, '1024': 575, '1000': 576, 'decim': 577, 'memori': 578, 'approxim': 579, 'notic': 580, '32': 581, 'closer': 582, 'rather': 583, 'reduc': 584, 'yn': 585, '100': 586, 'anyon': 587, 'read': 588, 'public': 589, 'idea': 590, 'abl': 591, 'phrase': 592, 'chapter': 593, 'demorgan': 594, '—q': 595, 'told': 596, 'pattern': 597, 'logic': 598, '22': 599, 'includ': 600, 'vz': 601, 'rewrit': 602, 'u1px': 603, 'u2qx': 604, 'tx': 605, 'z3i': 606, 'z3x': 607, 'uqi': 608, 'upz': 609, 'qz': 610, 'contraposit': 611, 'hose': 612, '60': 613, 'walk': 614, 'poem': 615, 'm2': 616, '2x': 617, 'disprov': 618, 'ohx': 619, 'explor': 620, 'iv': 621, '124': 622, '1n': 623, 'intuit': 624, 'union': 625, 'last': 626, 'weak': 627, 'directli': 628, 'abas': 629, 'end': 630, 'geometr': 631, 'rn': 632, '2tn2': 633, 'tn3': 634, 'asn': 635, 'fast': 636, 'grow': 637, 'behavior': 638, 'master': 639, 'log3': 640, 'tn2': 641, 'magicmiddl': 642, 'broke': 643, 'data': 644, 'select1': 645, '40': 646, 'insert': 647, 'exchang': 648, 'maximum': 649, 'describ': 650, 'succinctli': 651, 'school': 652, 'basebal': 653, 'tournament': 654, 'third': 655, 'secretarytreasur': 656, 'sell': 657, 'stomach': 658, 'decid': 659, 'disagre': 660, '11—the': 661, 'twice': 662, 'deli': 663, 'offer': 664, 'butter': 665, 'mayonnais': 666, 'spread': 667, 'unnecessari': 668, '113': 669, 'explain1': 670, 'imagin': 671, 'discret': 672, 'often': 673, 'fivedigit': 674, 'participants—four': 675, 'students—who': 676, 'lexicograph': 677, 'fiveel': 678, 'underlin': 679, 'rectangl': 680, 'professor': 681, '3hour': 682, 'lab': 683, 'hour': 684, 'done': 685, 'threeway': 686, 'tenni': 687, 'singl': 688, 'match': 689, 'serv': 690, 'basketbal': 691, 'realist': 692, 'normal': 693, 'skill': 694, 'fgi': 695, 'pascal': 696, 'triangl': 697, '15f': 698, 'y5': 699, '25': 700, 'y4': 701, 'chair': 702, 'green': 703, 'blue': 704, 'xk': 705, 'cartesian': 706, 'origin': 707, 'point': 708, 'built': 709, 'horizont': 710, 'vertic': 711, 'length': 712, '4element': 713, '20member': 714, 'vice': 715, 'secretari': 716, 'treasur': 717, 'threeperson': 718, '—o': 719, '°': 720, 'rememb': 721, 'se': 722, 'golden': 723, 'men': 724, 'women': 725, 'gender': 726, 'shelv': 727, 'bookcas': 728, 'push': 729, 'far': 730, 'shelf': 731, 'factori': 732, 'lead': 733, 'specif': 734, 'placement': 735, 'wood': 736, 'circl': 737, 'catalan': 738, 'particularli': 739, 'somewhat': 740, 'sophist': 741, 'standard': 742, 's0': 743, 'nonempti': 744, 'ksn': 745, 'like': 746, '90': 747, 'ie': 748, 'orient': 749, 'start': 750, 'clockwis': 751, 'square—that': 752, 'sy': 753, 'nk': 754, '01': 755, '2k': 756, 'krank': 757, 'offici': 758, 'govern': 759, 'necessarili': 760, 'rx': 761, '−4': 762, '♣': 763, '♦': 764, '♥': 765, '♠': 766, 'p·': 767, '⊆': 768, '4x': 769, '⊕': 770, 'venn': 771, 'method': 772, 'de': 773, 'morgan': 774, 'ac': 775, 'bc': 776, 'antisymmetr': 777, 'friend': 778, 'check': 779, 'suit': 780, '—11': 781, '9nat': 782, 'xnqqd': 783, 'rjxxflj': 784, '18': 785, '2318ph': 786, 'concaten': 787, 'six': 788, 'fewer': 789, 'unknown': 790, '913647': 791, '618232': 792, 'meaning': 793, '487': 794, 'z30031': 795, '13008': 796, 'appear': 797, '216': 798, 'saw': 799, '412': 800, 'z12': 801, 'pb': 802, 'fix': 803, 'obtain': 804, 'taken': 805, 'unencod': 806, '•7': 807, 'associ': 808, 'decod': 809, '26': 810, 'etc': 811, 'back': 812, '37': 813, '95': 814, 'process': 815, 'append': 816, '5s': 817, '•3122': 818, '•102': 819, '•nb': 820, '126': 821, 'gcdq': 822, 'cryptographi': 823, 'commun': 824, 'bug': 825, 'propos': 826, 'keep': 827, 'safe': 828, 'safe—that': 829, 'sound': 830, 'good': 831, 'complic': 832, 'qa': 833, 'qb': 834, 'fine': 835, 'gcd576': 836, '486': 837, '21': 838, 'z103': 839, 'sever': 840, 'input': 841, 'return': 842, 'expect': 843, 'lcm': 844, 'z10': 845, 'z11': 846, 'fourth': 847, '231111199': 848, 'z29in': 849, 'z43in': 850, '1051111199': 851, 'z29': 852, 'z43': 853, '1596': 854, 'z97': 855, '6772': 856, '6773': 857, 'a1': 858, 'interest': 859, 'zpq': 860, 'pq': 861, 'each—that': 862, 'bm': 863, 'substitut': 864, 'assert': 865, 'made': 866, 'denot': 867, 'chines': 868, 'remaind': 869, 'someon': 870, 'sake': 871, 'usual': 872, 'overflow': 873, 'slightli': 874, 'weaker': 875, 'infinit': 876, 'carmichael': 877, 'counterexamples213': 878, '31024': 879, 'a2': 880, 'a4': 881, 'a16': 882, 'a53': 883, 'eight': 884, '999': 885, '230': 886, '240': 887, 'compar': 888, 'reason': 889, 'programm': 890, '—1': 891, 'fill': 892, '10120th': 893, '10100': 894, '100digit': 895, 'a10120': 896, '10240': 897, 'lot': 898, 'depend': 899, 'outlin': 900, '241': 901, 'binari': 902, 'represent': 903, 'aeievem': 904, 'ax': 905, 'theorist': 906, 'ae1e2em': 907, 'ym': 908, 'zm': 909, '19': 910, 'zp1': 911, 'primal': 912, 'test': 913, 'xm1': 914, 'secur': 915, 'forg': 916, 'protect': 917, 'opposit': 918, 'rest': 919, 'world': 920, 'easili': 921, 'achiev': 922, 'tl': 923, 'mh': 924, 'v—t': 925, 'falset': 926, 'andor': 927, '—t': 928, 'exclus': 929, 'inclus': 930, 'twovari': 931, 'evalu': 932, 'contain': 933, 'procedur': 934, 'expand': 935, 'conveni': 936, 'introduc': 937, 'perhap': 938, '214': 939, 'algebra': 940, 'yz': 941, 'refer': 942, '6z': 943, '20ph': 944, '0bt': 945, '0ph': 946, 'zz2': 947, 'implicit': 948, 'negat': 949, 'slight': 950, 'modif': 951, '5x': 952, 'xy': 953, 'zqx': 954, 'pxmh': 955, 'zvi': 956, 'zsx': 957, 'ymh': 958, 'vy': 959, 'zqi': 960, 'zpx': 961, 'py': 962, 'u3i': 963, 'everyday': 964, 'life': 965, 'whatev': 966, 'prefer': 967, 'comment': 968, 'commut': 969, 'convers': 970, 'ft': 971, 'reach': 972, 'tomato': 973, 'georg': 974, 'mari': 975, 'pamela': 976, 'recit': 977, 'andr': 978, 'realli': 979, 'precis': 980, 'v3': 981, 'irrat': 982, 'ration': 983, 'okay': 984, 'necessari': 985, 'quadrat': 986, 'largest': 987, 'ogx': 988, 'gx': 989, 'ii': 990, 'iii': 991, '1a': 992, '23n': 993, 'vi': 994, 'assumpt': 995, 'vii': 996, 'viii': 997, 'pk': 998, 'j1': 999, 'nice': 1000, 'stori': 1001, 'worth': 1002, 'figur': 1003, 'a0': 1004, 'an1': 1005, 'amn': 1006, 'aman': 1007, 'argument': 1008, 'favor': 1009, 'quit': 1010, 'fact': 1011, 'occur': 1012, 'purpos': 1013, 'error': 1014, 'p1': 1015, 'therefor': 1016, '413': 1017, '416': 1018, '2mn': 1019, '3mn': 1020, 'oneel': 1021, 'deriv': 1022, '424': 1023, 'hatcheri': 1024, '2000': 1025, 'begin': 1026, 'doubl': 1027, 'due': 1028, 'reproduct': 1029, '3tn': 1030, '41': 1031, 'iter': 1032, 'seri': 1033, 'r2': 1034, 'ratio': 1035, 'n32n': 1036, '2f': 1037, '3n': 1038, 'r2n': 1039, '4tn4': 1040, '9tn3': 1041, '2tn4': 1042, 'n3f': 1043, 'ndat': 1044, '1pt': 1045, '421': 1046, 'logb': 1047, '0log2': 1048, 'constant': 1049, 'alogb': 1050, 'nlogba': 1051, 'break': 1052, 'per': 1053, 'level': 1054, 'nc': 1055, 'unit': 1056, 'yjn': 1057, 'found': 1058, 'hound': 1059, 'lcrm': 1060, 'atnb': 1061, 'asnb': 1062, 'influenc': 1063, '1ph': 1064, 'tn3tn2n312': 1065, 'n1': 1066, 'preliminari': 1067, '49': 1068, 'logyz': 1069, 'zlogyx': 1070, '2t': 1071, 'clog3': 1072, 'logarithm': 1073, 'still': 1074, 'attack': 1075, 'conclus': 1076, 'hold': 1077, '4t': 1078, 'got': 1079, 'tn2tn33n': 1080, 'inform': 1081, 'bad': 1082, 'n7': 1083, 'selectl': 1084, 'argu': 1085, 'tan': 1086, 'tbn': 1087, '468': 1088, 'tn6': 1089, 'dn': 1090, 'fn21': 1091, 'l': 1092, 'tn5': 1093, 'n5': 1094}\n",
      "(218, 50) (218, 4)\n",
      "(25, 50) (25, 4)\n"
     ]
    }
   ],
   "source": [
    "MAX_FEATURE = 0\n",
    "EMBED_SIZE = 300\n",
    "MAX_LENGTH = 50\n",
    "\n",
    "#Preparing the data\n",
    "doc = pd.read_excel('data.xlsx',header=0)\n",
    "doc = data_preprocessing(doc)\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_FEATURE, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\n",
    "tokenizer.fit_on_texts(doc[\"Question\"].values)\n",
    "MAX_FEATURE = len(tokenizer.word_index) + 1\n",
    "\n",
    "X = tokenizer.texts_to_sequences(doc[\"Question\"].values)\n",
    "X = pad_sequences(X, maxlen=MAX_LENGTH)\n",
    "Y = pd.get_dummies(doc[\"Label\"].values)\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.10, random_state = 42)\n",
    "\n",
    "#Preparing the models\n",
    "epochs = 100\n",
    "batch_size = 64\n",
    "\n",
    "RNNwithAttention = Sequential()\n",
    "RNNwithAttention.add(Embedding(MAX_FEATURE, EMBED_SIZE, input_length=X.shape[1]))\n",
    "RNNwithAttention.add(SpatialDropout1D(0.2))\n",
    "RNNwithAttention.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2,return_sequences=True))\n",
    "RNNwithAttention.add(AttentionWithContext())\n",
    "#RNNwithAttention.add(Dense(64, activation=\"swish\"))\n",
    "RNNwithAttention.add(Dense(4, activation='softmax'))\n",
    "RNNwithAttention.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "RNNwithAttention.save_weights('RNN with Attention Initializer.h5')\n",
    "\n",
    "\n",
    "\n",
    "RNN = Sequential()\n",
    "RNN.add(Embedding(MAX_FEATURE, EMBED_SIZE, input_length=X.shape[1]))\n",
    "RNN.add(SpatialDropout1D(0.2))\n",
    "RNN.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2))\n",
    "#RNN.add(AttentionWithContext())\n",
    "#RNN.add(Dense(64, activation=\"swish\"))\n",
    "RNN.add(Dense(4, activation='softmax'))\n",
    "RNN.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "RNN.save_weights('RNN Initializer.h5')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validatoin\n",
    "The following code performs cross validation with fold = 10 to the two models above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>let px stand “ x prime ” qx “ x even ” r x sta...</td>\n",
       "      <td>Propositional logic, Predicate logic, Inferenc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>find follow x 15f b x y5 c x 25 x — 15</td>\n",
       "      <td>Counting Lists, Permutations, and Subsets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>suppos want comput aeievem mod n discuss wheth...</td>\n",
       "      <td>Details of the RSA Cryptosystem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>suppos organ panel discuss allow alcohol campu...</td>\n",
       "      <td>Counting Lists, Permutations, and Subsets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>prove demorgan law state —p q —p v —q</td>\n",
       "      <td>Propositional logic, Predicate logic, Inferenc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>use sx z statement x yz x statement x form def...</td>\n",
       "      <td>Propositional logic, Predicate logic, Inferenc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>function g call invers function f domain g ran...</td>\n",
       "      <td>Counting Lists, Permutations, and Subsets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>extend proof preliminari version master theore...</td>\n",
       "      <td>The Master Theorem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>consid follow slight modif theorem 32 part eit...</td>\n",
       "      <td>Propositional logic, Predicate logic, Inferenc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>follow segment code part program use insert so...</td>\n",
       "      <td>Counting Lists, Permutations, and Subsets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>show induct solut recurr form tn 2t clog3 n n ...</td>\n",
       "      <td>The Master Theorem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>recal prime number divid product two integ div...</td>\n",
       "      <td>Details of the RSA Cryptosystem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>local ice cream shop sell ten differ flavor ic...</td>\n",
       "      <td>Counting Lists, Permutations, and Subsets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>let x ∈ n x 7 b x ∈ z x − 2 4 c x ∈ r x 3 − 4x...</td>\n",
       "      <td>Counting Lists, Permutations, and Subsets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>mani way tenperson club select presid twoperso...</td>\n",
       "      <td>Counting Lists, Permutations, and Subsets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>show logb n 0log2 n constant b 1</td>\n",
       "      <td>The Master Theorem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>given squar rotat 90 degre time ie squar four ...</td>\n",
       "      <td>Counting Lists, Permutations, and Subsets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>use induct prove 1 • 2 2 • 3 nn 1 nn 1n 23</td>\n",
       "      <td>The Master Theorem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>100digit number number digit a10120 closer 101...</td>\n",
       "      <td>Details of the RSA Cryptosystem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>determin whether everi nonzero element zn mult...</td>\n",
       "      <td>Details of the RSA Cryptosystem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>list element follow set x ∈ rx 2 5 b ∈ na −4 4...</td>\n",
       "      <td>Counting Lists, Permutations, and Subsets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>show p ® q equival p —q v —p q</td>\n",
       "      <td>Propositional logic, Predicate logic, Inferenc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>comput follow show explain work use calcul com...</td>\n",
       "      <td>Details of the RSA Cryptosystem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>follow express repres statement integ use px “...</td>\n",
       "      <td>Propositional logic, Predicate logic, Inferenc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>solv recurr tn 2tn — 1 3n t0 1</td>\n",
       "      <td>The Master Theorem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>give truth tabl follow express v —s v v — tl b...</td>\n",
       "      <td>Propositional logic, Predicate logic, Inferenc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>follow statement z stand posit integ z stand i...</td>\n",
       "      <td>Propositional logic, Predicate logic, Inferenc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>14 mod 9 — 1 mod 9 —11 mod 9nat</td>\n",
       "      <td>Details of the RSA Cryptosystem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>use venn diagram method prove second law de mo...</td>\n",
       "      <td>Counting Lists, Permutations, and Subsets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>mani way n peopl seat around round tabl rememb...</td>\n",
       "      <td>Counting Lists, Permutations, and Subsets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>• 133 — • 277 1 guarante invers mod</td>\n",
       "      <td>Details of the RSA Cryptosystem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>show prime least ym element zm multipl invers</td>\n",
       "      <td>Details of the RSA Cryptosystem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>find error follow “ proof ” posit integ n equa...</td>\n",
       "      <td>The Master Theorem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>mani base ten number five digit mani fivedigit...</td>\n",
       "      <td>Counting Lists, Permutations, and Subsets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>solv recurr mn 3mn — 1 1 abas case m1 1 differ...</td>\n",
       "      <td>The Master Theorem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>mani way draw first card second card deck 52 card</td>\n",
       "      <td>Counting Lists, Permutations, and Subsets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>short messag encod convert integ replac “ ” 1 ...</td>\n",
       "      <td>Details of the RSA Cryptosystem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>result six fewer digit unknown number ad messa...</td>\n",
       "      <td>Details of the RSA Cryptosystem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>recurs tree work even problem break geometr wo...</td>\n",
       "      <td>The Master Theorem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>check whether suit relat set 52 play card equi...</td>\n",
       "      <td>Counting Lists, Permutations, and Subsets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>show zp a1 mod p 1 mod p mod 1 mod p p prime</td>\n",
       "      <td>Details of the RSA Cryptosystem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>prove largest prime number</td>\n",
       "      <td>Propositional logic, Predicate logic, Inferenc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>construct contraposit proof real number x x2 —...</td>\n",
       "      <td>Propositional logic, Predicate logic, Inferenc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>give exampl english statement form vx e u3i e ...</td>\n",
       "      <td>Propositional logic, Predicate logic, Inferenc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>mani element multipl invers zpq p q prime</td>\n",
       "      <td>Details of the RSA Cryptosystem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>element set n nelement set mani order pair fir...</td>\n",
       "      <td>Counting Lists, Permutations, and Subsets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>local ice cream shop may get sunda two scoop i...</td>\n",
       "      <td>Counting Lists, Permutations, and Subsets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>suppos decid disagre mother problem 11—the ord...</td>\n",
       "      <td>Counting Lists, Permutations, and Subsets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>comput fourth power mod 5 element z5 observ ge...</td>\n",
       "      <td>Details of the RSA Cryptosystem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>standard notat number partit nelement set k cl...</td>\n",
       "      <td>Counting Lists, Permutations, and Subsets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>mani way draw two card deck 52 card</td>\n",
       "      <td>Counting Lists, Permutations, and Subsets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>mani way class 20 student choos group 3 studen...</td>\n",
       "      <td>Counting Lists, Permutations, and Subsets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>find big 0 bound best know get solut recurr tn...</td>\n",
       "      <td>The Master Theorem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>suppos c real number greater solut tn recurr t...</td>\n",
       "      <td>The Master Theorem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>consid recurr tn 3tn — 1 1 initi condit t0 2 c...</td>\n",
       "      <td>The Master Theorem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>draw recurs tree diagram 9tn3 n n 1 tn 1 n 1 u...</td>\n",
       "      <td>The Master Theorem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>big upper bound problem 3 actual big 0 bound</td>\n",
       "      <td>The Master Theorem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>use spreadsheet programm calcul comput find nu...</td>\n",
       "      <td>Details of the RSA Cryptosystem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>would mean say integ x equal 14 mod 9 meaning ...</td>\n",
       "      <td>Details of the RSA Cryptosystem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>number 29 43 prime 29 — 143 — 1 199 • 1111 z11...</td>\n",
       "      <td>Details of the RSA Cryptosystem</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>243 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Question  \\\n",
       "0    let px stand “ x prime ” qx “ x even ” r x sta...   \n",
       "1               find follow x 15f b x y5 c x 25 x — 15   \n",
       "2    suppos want comput aeievem mod n discuss wheth...   \n",
       "3    suppos organ panel discuss allow alcohol campu...   \n",
       "4                prove demorgan law state —p q —p v —q   \n",
       "5    use sx z statement x yz x statement x form def...   \n",
       "6    function g call invers function f domain g ran...   \n",
       "7    extend proof preliminari version master theore...   \n",
       "8    consid follow slight modif theorem 32 part eit...   \n",
       "9    follow segment code part program use insert so...   \n",
       "10   show induct solut recurr form tn 2t clog3 n n ...   \n",
       "11   recal prime number divid product two integ div...   \n",
       "12   local ice cream shop sell ten differ flavor ic...   \n",
       "13   let x ∈ n x 7 b x ∈ z x − 2 4 c x ∈ r x 3 − 4x...   \n",
       "14   mani way tenperson club select presid twoperso...   \n",
       "15                    show logb n 0log2 n constant b 1   \n",
       "16   given squar rotat 90 degre time ie squar four ...   \n",
       "17          use induct prove 1 • 2 2 • 3 nn 1 nn 1n 23   \n",
       "18   100digit number number digit a10120 closer 101...   \n",
       "19   determin whether everi nonzero element zn mult...   \n",
       "20   list element follow set x ∈ rx 2 5 b ∈ na −4 4...   \n",
       "21                      show p ® q equival p —q v —p q   \n",
       "22   comput follow show explain work use calcul com...   \n",
       "23   follow express repres statement integ use px “...   \n",
       "24                      solv recurr tn 2tn — 1 3n t0 1   \n",
       "25   give truth tabl follow express v —s v v — tl b...   \n",
       "26   follow statement z stand posit integ z stand i...   \n",
       "27                     14 mod 9 — 1 mod 9 —11 mod 9nat   \n",
       "28   use venn diagram method prove second law de mo...   \n",
       "29   mani way n peopl seat around round tabl rememb...   \n",
       "..                                                 ...   \n",
       "213                • 133 — • 277 1 guarante invers mod   \n",
       "214      show prime least ym element zm multipl invers   \n",
       "215  find error follow “ proof ” posit integ n equa...   \n",
       "216  mani base ten number five digit mani fivedigit...   \n",
       "217  solv recurr mn 3mn — 1 1 abas case m1 1 differ...   \n",
       "218  mani way draw first card second card deck 52 card   \n",
       "219  short messag encod convert integ replac “ ” 1 ...   \n",
       "220  result six fewer digit unknown number ad messa...   \n",
       "221  recurs tree work even problem break geometr wo...   \n",
       "222  check whether suit relat set 52 play card equi...   \n",
       "223       show zp a1 mod p 1 mod p mod 1 mod p p prime   \n",
       "224                         prove largest prime number   \n",
       "225  construct contraposit proof real number x x2 —...   \n",
       "226  give exampl english statement form vx e u3i e ...   \n",
       "227          mani element multipl invers zpq p q prime   \n",
       "228  element set n nelement set mani order pair fir...   \n",
       "229  local ice cream shop may get sunda two scoop i...   \n",
       "230  suppos decid disagre mother problem 11—the ord...   \n",
       "231  comput fourth power mod 5 element z5 observ ge...   \n",
       "232  standard notat number partit nelement set k cl...   \n",
       "233                mani way draw two card deck 52 card   \n",
       "234  mani way class 20 student choos group 3 studen...   \n",
       "235  find big 0 bound best know get solut recurr tn...   \n",
       "236  suppos c real number greater solut tn recurr t...   \n",
       "237  consid recurr tn 3tn — 1 1 initi condit t0 2 c...   \n",
       "238  draw recurs tree diagram 9tn3 n n 1 tn 1 n 1 u...   \n",
       "239       big upper bound problem 3 actual big 0 bound   \n",
       "240  use spreadsheet programm calcul comput find nu...   \n",
       "241  would mean say integ x equal 14 mod 9 meaning ...   \n",
       "242  number 29 43 prime 29 — 143 — 1 199 • 1111 z11...   \n",
       "\n",
       "                                                 Label  \n",
       "0    Propositional logic, Predicate logic, Inferenc...  \n",
       "1            Counting Lists, Permutations, and Subsets  \n",
       "2                      Details of the RSA Cryptosystem  \n",
       "3            Counting Lists, Permutations, and Subsets  \n",
       "4    Propositional logic, Predicate logic, Inferenc...  \n",
       "5    Propositional logic, Predicate logic, Inferenc...  \n",
       "6            Counting Lists, Permutations, and Subsets  \n",
       "7                                   The Master Theorem  \n",
       "8    Propositional logic, Predicate logic, Inferenc...  \n",
       "9            Counting Lists, Permutations, and Subsets  \n",
       "10                                  The Master Theorem  \n",
       "11                     Details of the RSA Cryptosystem  \n",
       "12           Counting Lists, Permutations, and Subsets  \n",
       "13           Counting Lists, Permutations, and Subsets  \n",
       "14           Counting Lists, Permutations, and Subsets  \n",
       "15                                  The Master Theorem  \n",
       "16           Counting Lists, Permutations, and Subsets  \n",
       "17                                  The Master Theorem  \n",
       "18                     Details of the RSA Cryptosystem  \n",
       "19                     Details of the RSA Cryptosystem  \n",
       "20           Counting Lists, Permutations, and Subsets  \n",
       "21   Propositional logic, Predicate logic, Inferenc...  \n",
       "22                     Details of the RSA Cryptosystem  \n",
       "23   Propositional logic, Predicate logic, Inferenc...  \n",
       "24                                  The Master Theorem  \n",
       "25   Propositional logic, Predicate logic, Inferenc...  \n",
       "26   Propositional logic, Predicate logic, Inferenc...  \n",
       "27                     Details of the RSA Cryptosystem  \n",
       "28           Counting Lists, Permutations, and Subsets  \n",
       "29           Counting Lists, Permutations, and Subsets  \n",
       "..                                                 ...  \n",
       "213                    Details of the RSA Cryptosystem  \n",
       "214                    Details of the RSA Cryptosystem  \n",
       "215                                 The Master Theorem  \n",
       "216          Counting Lists, Permutations, and Subsets  \n",
       "217                                 The Master Theorem  \n",
       "218          Counting Lists, Permutations, and Subsets  \n",
       "219                    Details of the RSA Cryptosystem  \n",
       "220                    Details of the RSA Cryptosystem  \n",
       "221                                 The Master Theorem  \n",
       "222          Counting Lists, Permutations, and Subsets  \n",
       "223                    Details of the RSA Cryptosystem  \n",
       "224  Propositional logic, Predicate logic, Inferenc...  \n",
       "225  Propositional logic, Predicate logic, Inferenc...  \n",
       "226  Propositional logic, Predicate logic, Inferenc...  \n",
       "227                    Details of the RSA Cryptosystem  \n",
       "228          Counting Lists, Permutations, and Subsets  \n",
       "229          Counting Lists, Permutations, and Subsets  \n",
       "230          Counting Lists, Permutations, and Subsets  \n",
       "231                    Details of the RSA Cryptosystem  \n",
       "232          Counting Lists, Permutations, and Subsets  \n",
       "233          Counting Lists, Permutations, and Subsets  \n",
       "234          Counting Lists, Permutations, and Subsets  \n",
       "235                                 The Master Theorem  \n",
       "236                                 The Master Theorem  \n",
       "237                                 The Master Theorem  \n",
       "238                                 The Master Theorem  \n",
       "239                                 The Master Theorem  \n",
       "240                    Details of the RSA Cryptosystem  \n",
       "241                    Details of the RSA Cryptosystem  \n",
       "242                    Details of the RSA Cryptosystem  \n",
       "\n",
       "[243 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 218 samples, validate on 25 samples\n",
      "Epoch 1/100\n",
      "218/218 [==============================] - 1s 4ms/step - loss: 1.3805 - accuracy: 0.3349 - val_loss: 1.3644 - val_accuracy: 0.4800\n",
      "Epoch 2/100\n",
      "218/218 [==============================] - 0s 747us/step - loss: 1.3330 - accuracy: 0.6284 - val_loss: 1.3371 - val_accuracy: 0.5600\n",
      "Epoch 3/100\n",
      "218/218 [==============================] - 0s 725us/step - loss: 1.2777 - accuracy: 0.6560 - val_loss: 1.2996 - val_accuracy: 0.5200\n",
      "Epoch 4/100\n",
      "218/218 [==============================] - 0s 717us/step - loss: 1.1975 - accuracy: 0.6560 - val_loss: 1.2531 - val_accuracy: 0.4800\n",
      "Epoch 5/100\n",
      "218/218 [==============================] - 0s 718us/step - loss: 1.0961 - accuracy: 0.6927 - val_loss: 1.1875 - val_accuracy: 0.5200\n",
      "Epoch 6/100\n",
      "218/218 [==============================] - 0s 722us/step - loss: 0.9688 - accuracy: 0.7339 - val_loss: 1.0982 - val_accuracy: 0.5600\n",
      "Epoch 7/100\n",
      "218/218 [==============================] - 0s 720us/step - loss: 0.8156 - accuracy: 0.7936 - val_loss: 1.0065 - val_accuracy: 0.6400\n",
      "Epoch 8/100\n",
      "218/218 [==============================] - 0s 727us/step - loss: 0.6761 - accuracy: 0.7890 - val_loss: 0.8882 - val_accuracy: 0.6400\n",
      "Epoch 9/100\n",
      "218/218 [==============================] - 0s 719us/step - loss: 0.5269 - accuracy: 0.8257 - val_loss: 0.8350 - val_accuracy: 0.6800\n",
      "Epoch 10/100\n",
      "218/218 [==============================] - 0s 722us/step - loss: 0.4470 - accuracy: 0.8486 - val_loss: 0.8320 - val_accuracy: 0.6800\n",
      "Epoch 11/100\n",
      "218/218 [==============================] - 0s 717us/step - loss: 0.3668 - accuracy: 0.8578 - val_loss: 0.7901 - val_accuracy: 0.7200\n",
      "Epoch 12/100\n",
      "218/218 [==============================] - 0s 711us/step - loss: 0.2879 - accuracy: 0.9220 - val_loss: 0.8135 - val_accuracy: 0.7600\n",
      "Epoch 13/100\n",
      "218/218 [==============================] - 0s 722us/step - loss: 0.2568 - accuracy: 0.9220 - val_loss: 0.8387 - val_accuracy: 0.7600\n",
      "Epoch 14/100\n",
      "218/218 [==============================] - 0s 717us/step - loss: 0.2111 - accuracy: 0.9679 - val_loss: 0.8403 - val_accuracy: 0.7600\n",
      "Train on 218 samples, validate on 25 samples\n",
      "Epoch 1/100\n",
      "218/218 [==============================] - 0s 731us/step - loss: 1.3831 - accuracy: 0.3440 - val_loss: 1.3814 - val_accuracy: 0.4000\n",
      "Epoch 2/100\n",
      "218/218 [==============================] - 0s 713us/step - loss: 1.3653 - accuracy: 0.5917 - val_loss: 1.3664 - val_accuracy: 0.6000\n",
      "Epoch 3/100\n",
      "218/218 [==============================] - 0s 729us/step - loss: 1.3406 - accuracy: 0.8303 - val_loss: 1.3493 - val_accuracy: 0.6800\n",
      "Epoch 4/100\n",
      "218/218 [==============================] - 0s 718us/step - loss: 1.3092 - accuracy: 0.9174 - val_loss: 1.3298 - val_accuracy: 0.7600\n",
      "Epoch 5/100\n",
      "218/218 [==============================] - 0s 730us/step - loss: 1.2718 - accuracy: 0.9220 - val_loss: 1.3065 - val_accuracy: 0.7200\n",
      "Epoch 6/100\n",
      "218/218 [==============================] - 0s 709us/step - loss: 1.2242 - accuracy: 0.9083 - val_loss: 1.2780 - val_accuracy: 0.7200\n",
      "Epoch 7/100\n",
      "218/218 [==============================] - 0s 722us/step - loss: 1.1651 - accuracy: 0.9128 - val_loss: 1.2403 - val_accuracy: 0.6800\n",
      "Epoch 8/100\n",
      "218/218 [==============================] - 0s 724us/step - loss: 1.0789 - accuracy: 0.9128 - val_loss: 1.1882 - val_accuracy: 0.6400\n",
      "Epoch 9/100\n",
      "218/218 [==============================] - 0s 726us/step - loss: 0.9764 - accuracy: 0.9083 - val_loss: 1.1171 - val_accuracy: 0.5200\n",
      "Epoch 10/100\n",
      "218/218 [==============================] - 0s 728us/step - loss: 0.8347 - accuracy: 0.8899 - val_loss: 1.0309 - val_accuracy: 0.5200\n",
      "Epoch 11/100\n",
      "218/218 [==============================] - 0s 714us/step - loss: 0.6885 - accuracy: 0.8670 - val_loss: 0.9164 - val_accuracy: 0.6000\n",
      "Epoch 12/100\n",
      "218/218 [==============================] - 0s 720us/step - loss: 0.5483 - accuracy: 0.8807 - val_loss: 0.7772 - val_accuracy: 0.6800\n",
      "Epoch 13/100\n",
      "218/218 [==============================] - 0s 714us/step - loss: 0.4295 - accuracy: 0.9312 - val_loss: 0.6755 - val_accuracy: 0.8000\n",
      "Epoch 14/100\n",
      "218/218 [==============================] - 0s 723us/step - loss: 0.3058 - accuracy: 0.9633 - val_loss: 0.6105 - val_accuracy: 0.8000\n",
      "Epoch 15/100\n",
      "218/218 [==============================] - 0s 714us/step - loss: 0.2186 - accuracy: 0.9862 - val_loss: 0.5583 - val_accuracy: 0.8800\n",
      "Epoch 16/100\n",
      "218/218 [==============================] - 0s 726us/step - loss: 0.1616 - accuracy: 0.9954 - val_loss: 0.4543 - val_accuracy: 0.9600\n",
      "Epoch 17/100\n",
      "218/218 [==============================] - 0s 723us/step - loss: 0.1054 - accuracy: 0.9954 - val_loss: 0.4357 - val_accuracy: 0.9200\n",
      "Epoch 18/100\n",
      "218/218 [==============================] - 0s 715us/step - loss: 0.0845 - accuracy: 0.9908 - val_loss: 0.3825 - val_accuracy: 0.9600\n",
      "Epoch 19/100\n",
      "218/218 [==============================] - 0s 721us/step - loss: 0.0626 - accuracy: 0.9908 - val_loss: 0.3234 - val_accuracy: 0.9600\n",
      "Epoch 20/100\n",
      "218/218 [==============================] - 0s 719us/step - loss: 0.0560 - accuracy: 0.9954 - val_loss: 0.3112 - val_accuracy: 0.9600\n",
      "Epoch 21/100\n",
      "218/218 [==============================] - 0s 735us/step - loss: 0.0331 - accuracy: 1.0000 - val_loss: 0.3196 - val_accuracy: 0.9600\n",
      "Epoch 22/100\n",
      "218/218 [==============================] - 0s 726us/step - loss: 0.0320 - accuracy: 1.0000 - val_loss: 0.3280 - val_accuracy: 0.8800\n",
      "Epoch 23/100\n",
      "218/218 [==============================] - 0s 721us/step - loss: 0.0256 - accuracy: 1.0000 - val_loss: 0.3257 - val_accuracy: 0.8800\n",
      "Train on 218 samples, validate on 25 samples\n",
      "Epoch 1/100\n",
      "218/218 [==============================] - 0s 718us/step - loss: 1.3841 - accuracy: 0.3028 - val_loss: 1.3809 - val_accuracy: 0.4400\n",
      "Epoch 2/100\n",
      "218/218 [==============================] - 0s 727us/step - loss: 1.3734 - accuracy: 0.4725 - val_loss: 1.3710 - val_accuracy: 0.4800\n",
      "Epoch 3/100\n",
      "218/218 [==============================] - 0s 721us/step - loss: 1.3586 - accuracy: 0.6651 - val_loss: 1.3585 - val_accuracy: 0.5200\n",
      "Epoch 4/100\n",
      "218/218 [==============================] - 0s 728us/step - loss: 1.3350 - accuracy: 0.7064 - val_loss: 1.3441 - val_accuracy: 0.5200\n",
      "Epoch 5/100\n",
      "218/218 [==============================] - 0s 713us/step - loss: 1.3068 - accuracy: 0.7890 - val_loss: 1.3271 - val_accuracy: 0.5200\n",
      "Epoch 6/100\n",
      "218/218 [==============================] - 0s 727us/step - loss: 1.2764 - accuracy: 0.7936 - val_loss: 1.3057 - val_accuracy: 0.6000\n",
      "Epoch 7/100\n",
      "218/218 [==============================] - 0s 724us/step - loss: 1.2366 - accuracy: 0.8073 - val_loss: 1.2790 - val_accuracy: 0.6000\n",
      "Epoch 8/100\n",
      "218/218 [==============================] - 0s 724us/step - loss: 1.1842 - accuracy: 0.8073 - val_loss: 1.2440 - val_accuracy: 0.6400\n",
      "Epoch 9/100\n",
      "218/218 [==============================] - 0s 725us/step - loss: 1.1161 - accuracy: 0.8119 - val_loss: 1.1967 - val_accuracy: 0.6800\n",
      "Epoch 10/100\n",
      "218/218 [==============================] - 0s 715us/step - loss: 1.0190 - accuracy: 0.8073 - val_loss: 1.1314 - val_accuracy: 0.6800\n",
      "Epoch 11/100\n",
      "218/218 [==============================] - 0s 720us/step - loss: 0.8993 - accuracy: 0.8028 - val_loss: 1.0491 - val_accuracy: 0.6400\n",
      "Epoch 12/100\n",
      "218/218 [==============================] - 0s 711us/step - loss: 0.7808 - accuracy: 0.7844 - val_loss: 0.9586 - val_accuracy: 0.6400\n",
      "Epoch 13/100\n",
      "218/218 [==============================] - 0s 720us/step - loss: 0.6444 - accuracy: 0.7844 - val_loss: 0.8615 - val_accuracy: 0.6800\n",
      "Epoch 14/100\n",
      "218/218 [==============================] - 0s 729us/step - loss: 0.5086 - accuracy: 0.8073 - val_loss: 0.7758 - val_accuracy: 0.6800\n",
      "Epoch 15/100\n",
      "218/218 [==============================] - 0s 725us/step - loss: 0.4146 - accuracy: 0.8670 - val_loss: 0.7157 - val_accuracy: 0.7600\n",
      "Epoch 16/100\n",
      "218/218 [==============================] - 0s 728us/step - loss: 0.3323 - accuracy: 0.9037 - val_loss: 0.7114 - val_accuracy: 0.8000\n",
      "Epoch 17/100\n",
      "218/218 [==============================] - 0s 728us/step - loss: 0.2688 - accuracy: 0.8761 - val_loss: 0.6554 - val_accuracy: 0.8000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/100\n",
      "218/218 [==============================] - 0s 723us/step - loss: 0.2122 - accuracy: 0.9725 - val_loss: 0.7102 - val_accuracy: 0.7600\n",
      "Epoch 19/100\n",
      "218/218 [==============================] - 0s 724us/step - loss: 0.1729 - accuracy: 0.9771 - val_loss: 0.7764 - val_accuracy: 0.8000\n",
      "Epoch 20/100\n",
      "218/218 [==============================] - 0s 719us/step - loss: 0.1608 - accuracy: 0.9771 - val_loss: 0.7134 - val_accuracy: 0.8400\n",
      "Train on 218 samples, validate on 25 samples\n",
      "Epoch 1/100\n",
      "218/218 [==============================] - 0s 731us/step - loss: 1.3853 - accuracy: 0.2385 - val_loss: 1.3738 - val_accuracy: 0.5600\n",
      "Epoch 2/100\n",
      "218/218 [==============================] - 0s 724us/step - loss: 1.3694 - accuracy: 0.5596 - val_loss: 1.3590 - val_accuracy: 0.7200\n",
      "Epoch 3/100\n",
      "218/218 [==============================] - 0s 720us/step - loss: 1.3480 - accuracy: 0.7064 - val_loss: 1.3409 - val_accuracy: 0.7600\n",
      "Epoch 4/100\n",
      "218/218 [==============================] - 0s 721us/step - loss: 1.3200 - accuracy: 0.8670 - val_loss: 1.3186 - val_accuracy: 0.7600\n",
      "Epoch 5/100\n",
      "218/218 [==============================] - 0s 722us/step - loss: 1.2889 - accuracy: 0.8807 - val_loss: 1.2917 - val_accuracy: 0.8000\n",
      "Epoch 6/100\n",
      "218/218 [==============================] - 0s 727us/step - loss: 1.2477 - accuracy: 0.9174 - val_loss: 1.2582 - val_accuracy: 0.6800\n",
      "Epoch 7/100\n",
      "218/218 [==============================] - 0s 726us/step - loss: 1.1950 - accuracy: 0.9083 - val_loss: 1.2159 - val_accuracy: 0.6800\n",
      "Epoch 8/100\n",
      "218/218 [==============================] - 0s 725us/step - loss: 1.1315 - accuracy: 0.9037 - val_loss: 1.1593 - val_accuracy: 0.6800\n",
      "Epoch 9/100\n",
      "218/218 [==============================] - 0s 728us/step - loss: 1.0419 - accuracy: 0.9128 - val_loss: 1.0823 - val_accuracy: 0.7200\n",
      "Epoch 10/100\n",
      "218/218 [==============================] - 0s 724us/step - loss: 0.9205 - accuracy: 0.9083 - val_loss: 0.9771 - val_accuracy: 0.7600\n",
      "Epoch 11/100\n",
      "218/218 [==============================] - 0s 722us/step - loss: 0.7711 - accuracy: 0.9128 - val_loss: 0.8479 - val_accuracy: 0.8000\n",
      "Epoch 12/100\n",
      "218/218 [==============================] - 0s 718us/step - loss: 0.6209 - accuracy: 0.9174 - val_loss: 0.7091 - val_accuracy: 0.8400\n",
      "Epoch 13/100\n",
      "218/218 [==============================] - 0s 722us/step - loss: 0.4650 - accuracy: 0.8991 - val_loss: 0.6203 - val_accuracy: 0.8000\n",
      "Epoch 14/100\n",
      "218/218 [==============================] - 0s 725us/step - loss: 0.3505 - accuracy: 0.9312 - val_loss: 0.4987 - val_accuracy: 0.8400\n",
      "Epoch 15/100\n",
      "218/218 [==============================] - 0s 708us/step - loss: 0.2410 - accuracy: 0.9771 - val_loss: 0.4407 - val_accuracy: 0.8800\n",
      "Epoch 16/100\n",
      "218/218 [==============================] - 0s 717us/step - loss: 0.1756 - accuracy: 0.9771 - val_loss: 0.3657 - val_accuracy: 0.9600\n",
      "Epoch 17/100\n",
      "218/218 [==============================] - 0s 730us/step - loss: 0.1123 - accuracy: 0.9954 - val_loss: 0.2980 - val_accuracy: 0.9200\n",
      "Epoch 18/100\n",
      "218/218 [==============================] - 0s 712us/step - loss: 0.0889 - accuracy: 0.9954 - val_loss: 0.2659 - val_accuracy: 0.9200\n",
      "Epoch 19/100\n",
      "218/218 [==============================] - 0s 721us/step - loss: 0.0640 - accuracy: 0.9954 - val_loss: 0.2494 - val_accuracy: 0.8800\n",
      "Epoch 20/100\n",
      "218/218 [==============================] - 0s 726us/step - loss: 0.0409 - accuracy: 1.0000 - val_loss: 0.2533 - val_accuracy: 0.8800\n",
      "Epoch 21/100\n",
      "218/218 [==============================] - 0s 716us/step - loss: 0.0363 - accuracy: 1.0000 - val_loss: 0.2464 - val_accuracy: 0.8800\n",
      "Epoch 22/100\n",
      "218/218 [==============================] - 0s 719us/step - loss: 0.0257 - accuracy: 1.0000 - val_loss: 0.2320 - val_accuracy: 0.8800\n",
      "Epoch 23/100\n",
      "218/218 [==============================] - 0s 723us/step - loss: 0.0213 - accuracy: 1.0000 - val_loss: 0.2148 - val_accuracy: 0.8800\n",
      "Epoch 24/100\n",
      "218/218 [==============================] - 0s 710us/step - loss: 0.0202 - accuracy: 1.0000 - val_loss: 0.2044 - val_accuracy: 0.8800\n",
      "Epoch 25/100\n",
      "218/218 [==============================] - 0s 725us/step - loss: 0.0150 - accuracy: 1.0000 - val_loss: 0.2007 - val_accuracy: 0.8800\n",
      "Epoch 26/100\n",
      "218/218 [==============================] - 0s 716us/step - loss: 0.0132 - accuracy: 1.0000 - val_loss: 0.1992 - val_accuracy: 0.9200\n",
      "Epoch 27/100\n",
      "218/218 [==============================] - 0s 729us/step - loss: 0.0126 - accuracy: 1.0000 - val_loss: 0.2002 - val_accuracy: 0.8800\n",
      "Epoch 28/100\n",
      "218/218 [==============================] - 0s 716us/step - loss: 0.0097 - accuracy: 1.0000 - val_loss: 0.2037 - val_accuracy: 0.8800\n",
      "Epoch 29/100\n",
      "218/218 [==============================] - 0s 716us/step - loss: 0.0122 - accuracy: 1.0000 - val_loss: 0.2030 - val_accuracy: 0.8800\n",
      "Train on 219 samples, validate on 24 samples\n",
      "Epoch 1/100\n",
      "219/219 [==============================] - 0s 714us/step - loss: 1.3861 - accuracy: 0.2603 - val_loss: 1.3838 - val_accuracy: 0.2500\n",
      "Epoch 2/100\n",
      "219/219 [==============================] - 0s 722us/step - loss: 1.3766 - accuracy: 0.4338 - val_loss: 1.3753 - val_accuracy: 0.5833\n",
      "Epoch 3/100\n",
      "219/219 [==============================] - 0s 721us/step - loss: 1.3596 - accuracy: 0.6119 - val_loss: 1.3637 - val_accuracy: 0.5833\n",
      "Epoch 4/100\n",
      "219/219 [==============================] - 0s 725us/step - loss: 1.3412 - accuracy: 0.6804 - val_loss: 1.3502 - val_accuracy: 0.6250\n",
      "Epoch 5/100\n",
      "219/219 [==============================] - 0s 717us/step - loss: 1.3182 - accuracy: 0.6986 - val_loss: 1.3344 - val_accuracy: 0.6667\n",
      "Epoch 6/100\n",
      "219/219 [==============================] - 0s 716us/step - loss: 1.2902 - accuracy: 0.7534 - val_loss: 1.3144 - val_accuracy: 0.6667\n",
      "Epoch 7/100\n",
      "219/219 [==============================] - 0s 713us/step - loss: 1.2531 - accuracy: 0.7580 - val_loss: 1.2895 - val_accuracy: 0.6667\n",
      "Epoch 8/100\n",
      "219/219 [==============================] - 0s 711us/step - loss: 1.2099 - accuracy: 0.7900 - val_loss: 1.2565 - val_accuracy: 0.6250\n",
      "Epoch 9/100\n",
      "219/219 [==============================] - 0s 716us/step - loss: 1.1483 - accuracy: 0.7991 - val_loss: 1.2103 - val_accuracy: 0.6250\n",
      "Epoch 10/100\n",
      "219/219 [==============================] - 0s 728us/step - loss: 1.0653 - accuracy: 0.8219 - val_loss: 1.1464 - val_accuracy: 0.6667\n",
      "Epoch 11/100\n",
      "219/219 [==============================] - 0s 717us/step - loss: 0.9571 - accuracy: 0.8037 - val_loss: 1.0583 - val_accuracy: 0.7083\n",
      "Epoch 12/100\n",
      "219/219 [==============================] - 0s 726us/step - loss: 0.8212 - accuracy: 0.8037 - val_loss: 0.9490 - val_accuracy: 0.6667\n",
      "Epoch 13/100\n",
      "219/219 [==============================] - 0s 724us/step - loss: 0.6730 - accuracy: 0.8174 - val_loss: 0.8320 - val_accuracy: 0.7083\n",
      "Epoch 14/100\n",
      "219/219 [==============================] - 0s 714us/step - loss: 0.5369 - accuracy: 0.8174 - val_loss: 0.7348 - val_accuracy: 0.7083\n",
      "Epoch 15/100\n",
      "219/219 [==============================] - 0s 731us/step - loss: 0.4602 - accuracy: 0.8128 - val_loss: 0.7180 - val_accuracy: 0.7083\n",
      "Epoch 16/100\n",
      "219/219 [==============================] - 0s 720us/step - loss: 0.3556 - accuracy: 0.8630 - val_loss: 0.7188 - val_accuracy: 0.7500\n",
      "Epoch 17/100\n",
      "219/219 [==============================] - 0s 727us/step - loss: 0.3207 - accuracy: 0.8950 - val_loss: 0.7301 - val_accuracy: 0.6667\n",
      "Epoch 18/100\n",
      "219/219 [==============================] - 0s 720us/step - loss: 0.2541 - accuracy: 0.8904 - val_loss: 0.7276 - val_accuracy: 0.6667\n",
      "Train on 219 samples, validate on 24 samples\n",
      "Epoch 1/100\n",
      "219/219 [==============================] - 0s 716us/step - loss: 1.3826 - accuracy: 0.3151 - val_loss: 1.3687 - val_accuracy: 0.5833\n",
      "Epoch 2/100\n",
      "219/219 [==============================] - 0s 726us/step - loss: 1.3611 - accuracy: 0.6119 - val_loss: 1.3496 - val_accuracy: 0.6667\n",
      "Epoch 3/100\n",
      "219/219 [==============================] - 0s 727us/step - loss: 1.3338 - accuracy: 0.7534 - val_loss: 1.3270 - val_accuracy: 0.5833\n",
      "Epoch 4/100\n",
      "219/219 [==============================] - 0s 723us/step - loss: 1.2989 - accuracy: 0.8082 - val_loss: 1.3002 - val_accuracy: 0.5833\n",
      "Epoch 5/100\n",
      "219/219 [==============================] - 0s 717us/step - loss: 1.2569 - accuracy: 0.8402 - val_loss: 1.2678 - val_accuracy: 0.6667\n",
      "Epoch 6/100\n",
      "219/219 [==============================] - 0s 715us/step - loss: 1.2008 - accuracy: 0.8584 - val_loss: 1.2255 - val_accuracy: 0.7083\n",
      "Epoch 7/100\n",
      "219/219 [==============================] - 0s 724us/step - loss: 1.1276 - accuracy: 0.8630 - val_loss: 1.1669 - val_accuracy: 0.7917\n",
      "Epoch 8/100\n",
      "219/219 [==============================] - 0s 721us/step - loss: 1.0300 - accuracy: 0.8721 - val_loss: 1.0836 - val_accuracy: 0.7500\n",
      "Epoch 9/100\n",
      "219/219 [==============================] - 0s 718us/step - loss: 0.8924 - accuracy: 0.8858 - val_loss: 0.9668 - val_accuracy: 0.7500\n",
      "Epoch 10/100\n",
      "219/219 [==============================] - 0s 712us/step - loss: 0.7324 - accuracy: 0.8950 - val_loss: 0.8155 - val_accuracy: 0.7917\n",
      "Epoch 11/100\n",
      "219/219 [==============================] - 0s 718us/step - loss: 0.5790 - accuracy: 0.8767 - val_loss: 0.6553 - val_accuracy: 0.7917\n",
      "Epoch 12/100\n",
      "219/219 [==============================] - 0s 718us/step - loss: 0.4320 - accuracy: 0.8995 - val_loss: 0.5406 - val_accuracy: 0.7917\n",
      "Epoch 13/100\n",
      "219/219 [==============================] - 0s 716us/step - loss: 0.3186 - accuracy: 0.9589 - val_loss: 0.4598 - val_accuracy: 0.9583\n",
      "Epoch 14/100\n",
      "219/219 [==============================] - 0s 717us/step - loss: 0.2367 - accuracy: 0.9772 - val_loss: 0.3971 - val_accuracy: 0.9583\n",
      "Epoch 15/100\n",
      "219/219 [==============================] - 0s 720us/step - loss: 0.1684 - accuracy: 0.9909 - val_loss: 0.3597 - val_accuracy: 0.9583\n",
      "Epoch 16/100\n",
      "219/219 [==============================] - 0s 712us/step - loss: 0.1184 - accuracy: 0.9954 - val_loss: 0.3356 - val_accuracy: 0.9583\n",
      "Epoch 17/100\n",
      "219/219 [==============================] - 0s 721us/step - loss: 0.0927 - accuracy: 1.0000 - val_loss: 0.3359 - val_accuracy: 0.9583\n",
      "Epoch 18/100\n",
      "219/219 [==============================] - 0s 715us/step - loss: 0.0867 - accuracy: 0.9909 - val_loss: 0.4481 - val_accuracy: 0.9167\n",
      "Epoch 19/100\n",
      "219/219 [==============================] - 0s 716us/step - loss: 0.1018 - accuracy: 0.9863 - val_loss: 0.4496 - val_accuracy: 0.8750\n",
      "Train on 219 samples, validate on 24 samples\n",
      "Epoch 1/100\n",
      "219/219 [==============================] - 0s 710us/step - loss: 1.3851 - accuracy: 0.2785 - val_loss: 1.3803 - val_accuracy: 0.3333\n",
      "Epoch 2/100\n",
      "219/219 [==============================] - 0s 717us/step - loss: 1.3733 - accuracy: 0.4429 - val_loss: 1.3701 - val_accuracy: 0.5417\n",
      "Epoch 3/100\n",
      "219/219 [==============================] - 0s 720us/step - loss: 1.3547 - accuracy: 0.6986 - val_loss: 1.3571 - val_accuracy: 0.5833\n",
      "Epoch 4/100\n",
      "219/219 [==============================] - 0s 719us/step - loss: 1.3315 - accuracy: 0.8219 - val_loss: 1.3418 - val_accuracy: 0.6250\n",
      "Epoch 5/100\n",
      "219/219 [==============================] - 0s 714us/step - loss: 1.3011 - accuracy: 0.8858 - val_loss: 1.3238 - val_accuracy: 0.6250\n",
      "Epoch 6/100\n",
      "219/219 [==============================] - 0s 716us/step - loss: 1.2649 - accuracy: 0.8767 - val_loss: 1.3014 - val_accuracy: 0.5833\n",
      "Epoch 7/100\n",
      "219/219 [==============================] - 0s 712us/step - loss: 1.2185 - accuracy: 0.8584 - val_loss: 1.2718 - val_accuracy: 0.5833\n",
      "Epoch 8/100\n",
      "219/219 [==============================] - 0s 719us/step - loss: 1.1610 - accuracy: 0.8447 - val_loss: 1.2316 - val_accuracy: 0.5833\n",
      "Epoch 9/100\n",
      "219/219 [==============================] - 0s 720us/step - loss: 1.0785 - accuracy: 0.8356 - val_loss: 1.1777 - val_accuracy: 0.6667\n",
      "Epoch 10/100\n",
      "219/219 [==============================] - 0s 722us/step - loss: 0.9750 - accuracy: 0.8356 - val_loss: 1.1066 - val_accuracy: 0.6667\n",
      "Epoch 11/100\n",
      "219/219 [==============================] - 0s 711us/step - loss: 0.8560 - accuracy: 0.8037 - val_loss: 1.0202 - val_accuracy: 0.6667\n",
      "Epoch 12/100\n",
      "219/219 [==============================] - 0s 726us/step - loss: 0.7220 - accuracy: 0.8128 - val_loss: 0.9201 - val_accuracy: 0.6667\n",
      "Epoch 13/100\n",
      "219/219 [==============================] - 0s 722us/step - loss: 0.5810 - accuracy: 0.8676 - val_loss: 0.8135 - val_accuracy: 0.6667\n",
      "Epoch 14/100\n",
      "219/219 [==============================] - 0s 718us/step - loss: 0.4614 - accuracy: 0.8767 - val_loss: 0.7003 - val_accuracy: 0.7083\n",
      "Epoch 15/100\n",
      "219/219 [==============================] - 0s 721us/step - loss: 0.3440 - accuracy: 0.9589 - val_loss: 0.6258 - val_accuracy: 0.7500\n",
      "Epoch 16/100\n",
      "219/219 [==============================] - 0s 726us/step - loss: 0.2366 - accuracy: 0.9817 - val_loss: 0.6220 - val_accuracy: 0.7500\n",
      "Epoch 17/100\n",
      "219/219 [==============================] - 0s 730us/step - loss: 0.1786 - accuracy: 0.9680 - val_loss: 0.5726 - val_accuracy: 0.7500\n",
      "Epoch 18/100\n",
      "219/219 [==============================] - 0s 722us/step - loss: 0.1235 - accuracy: 0.9863 - val_loss: 0.4769 - val_accuracy: 0.7500\n",
      "Epoch 19/100\n",
      "219/219 [==============================] - 0s 721us/step - loss: 0.1040 - accuracy: 0.9817 - val_loss: 0.4426 - val_accuracy: 0.7917\n",
      "Epoch 20/100\n",
      "219/219 [==============================] - 0s 718us/step - loss: 0.0700 - accuracy: 0.9954 - val_loss: 0.4668 - val_accuracy: 0.7917\n",
      "Epoch 21/100\n",
      "219/219 [==============================] - 0s 723us/step - loss: 0.0508 - accuracy: 1.0000 - val_loss: 0.5039 - val_accuracy: 0.8333\n",
      "Epoch 22/100\n",
      "219/219 [==============================] - 0s 719us/step - loss: 0.0439 - accuracy: 0.9954 - val_loss: 0.5253 - val_accuracy: 0.7500\n",
      "Train on 219 samples, validate on 24 samples\n",
      "Epoch 1/100\n",
      "219/219 [==============================] - 0s 714us/step - loss: 1.3842 - accuracy: 0.2922 - val_loss: 1.3816 - val_accuracy: 0.3333\n",
      "Epoch 2/100\n",
      "219/219 [==============================] - 0s 727us/step - loss: 1.3722 - accuracy: 0.4886 - val_loss: 1.3733 - val_accuracy: 0.4167\n",
      "Epoch 3/100\n",
      "219/219 [==============================] - 0s 705us/step - loss: 1.3536 - accuracy: 0.6621 - val_loss: 1.3628 - val_accuracy: 0.5833\n",
      "Epoch 4/100\n",
      "219/219 [==============================] - 0s 719us/step - loss: 1.3278 - accuracy: 0.7671 - val_loss: 1.3505 - val_accuracy: 0.5833\n",
      "Epoch 5/100\n",
      "219/219 [==============================] - 0s 706us/step - loss: 1.2996 - accuracy: 0.7900 - val_loss: 1.3357 - val_accuracy: 0.5833\n",
      "Epoch 6/100\n",
      "219/219 [==============================] - 0s 728us/step - loss: 1.2622 - accuracy: 0.7991 - val_loss: 1.3167 - val_accuracy: 0.6250\n",
      "Epoch 7/100\n",
      "219/219 [==============================] - 0s 715us/step - loss: 1.2205 - accuracy: 0.7991 - val_loss: 1.2922 - val_accuracy: 0.6250\n",
      "Epoch 8/100\n",
      "219/219 [==============================] - 0s 722us/step - loss: 1.1582 - accuracy: 0.8174 - val_loss: 1.2588 - val_accuracy: 0.6250\n",
      "Epoch 9/100\n",
      "219/219 [==============================] - 0s 715us/step - loss: 1.0829 - accuracy: 0.8219 - val_loss: 1.2149 - val_accuracy: 0.6667\n",
      "Epoch 10/100\n",
      "219/219 [==============================] - 0s 721us/step - loss: 0.9719 - accuracy: 0.8174 - val_loss: 1.1527 - val_accuracy: 0.6667\n",
      "Epoch 11/100\n",
      "219/219 [==============================] - 0s 711us/step - loss: 0.8429 - accuracy: 0.8174 - val_loss: 1.0779 - val_accuracy: 0.6250\n",
      "Epoch 12/100\n",
      "219/219 [==============================] - 0s 720us/step - loss: 0.7025 - accuracy: 0.7991 - val_loss: 0.9904 - val_accuracy: 0.6667\n",
      "Epoch 13/100\n",
      "219/219 [==============================] - 0s 716us/step - loss: 0.5742 - accuracy: 0.8265 - val_loss: 0.9018 - val_accuracy: 0.7083\n",
      "Epoch 14/100\n",
      "219/219 [==============================] - 0s 722us/step - loss: 0.4574 - accuracy: 0.8311 - val_loss: 0.8150 - val_accuracy: 0.7500\n",
      "Epoch 15/100\n",
      "219/219 [==============================] - 0s 715us/step - loss: 0.3584 - accuracy: 0.8950 - val_loss: 0.7028 - val_accuracy: 0.7500\n",
      "Epoch 16/100\n",
      "219/219 [==============================] - 0s 713us/step - loss: 0.2876 - accuracy: 0.9269 - val_loss: 0.6272 - val_accuracy: 0.7500\n",
      "Epoch 17/100\n",
      "219/219 [==============================] - 0s 726us/step - loss: 0.2097 - accuracy: 0.9726 - val_loss: 0.5781 - val_accuracy: 0.8333\n",
      "Epoch 18/100\n",
      "219/219 [==============================] - 0s 721us/step - loss: 0.1603 - accuracy: 0.9909 - val_loss: 0.5540 - val_accuracy: 0.8750\n",
      "Epoch 19/100\n",
      "219/219 [==============================] - 0s 720us/step - loss: 0.1306 - accuracy: 0.9954 - val_loss: 0.5328 - val_accuracy: 0.8333\n",
      "Epoch 20/100\n",
      "219/219 [==============================] - 0s 710us/step - loss: 0.1139 - accuracy: 0.9863 - val_loss: 0.5124 - val_accuracy: 0.8333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/100\n",
      "219/219 [==============================] - 0s 733us/step - loss: 0.0660 - accuracy: 1.0000 - val_loss: 0.4964 - val_accuracy: 0.8333\n",
      "Epoch 22/100\n",
      "219/219 [==============================] - 0s 734us/step - loss: 0.0456 - accuracy: 1.0000 - val_loss: 0.4860 - val_accuracy: 0.7917\n",
      "Epoch 23/100\n",
      "219/219 [==============================] - 0s 716us/step - loss: 0.0400 - accuracy: 1.0000 - val_loss: 0.4826 - val_accuracy: 0.7917\n",
      "Epoch 24/100\n",
      "219/219 [==============================] - 0s 723us/step - loss: 0.0309 - accuracy: 0.9954 - val_loss: 0.4851 - val_accuracy: 0.7917\n",
      "Epoch 25/100\n",
      "219/219 [==============================] - 0s 733us/step - loss: 0.0246 - accuracy: 1.0000 - val_loss: 0.4927 - val_accuracy: 0.7917\n",
      "Epoch 26/100\n",
      "219/219 [==============================] - 0s 720us/step - loss: 0.0226 - accuracy: 1.0000 - val_loss: 0.5088 - val_accuracy: 0.7917\n",
      "Train on 219 samples, validate on 24 samples\n",
      "Epoch 1/100\n",
      "219/219 [==============================] - 0s 721us/step - loss: 1.3853 - accuracy: 0.2740 - val_loss: 1.3788 - val_accuracy: 0.3333\n",
      "Epoch 2/100\n",
      "219/219 [==============================] - 0s 718us/step - loss: 1.3763 - accuracy: 0.4338 - val_loss: 1.3674 - val_accuracy: 0.5833\n",
      "Epoch 3/100\n",
      "219/219 [==============================] - 0s 705us/step - loss: 1.3607 - accuracy: 0.6256 - val_loss: 1.3531 - val_accuracy: 0.5833\n",
      "Epoch 4/100\n",
      "219/219 [==============================] - 0s 716us/step - loss: 1.3361 - accuracy: 0.7443 - val_loss: 1.3357 - val_accuracy: 0.5417\n",
      "Epoch 5/100\n",
      "219/219 [==============================] - 0s 713us/step - loss: 1.3139 - accuracy: 0.7397 - val_loss: 1.3153 - val_accuracy: 0.5833\n",
      "Epoch 6/100\n",
      "219/219 [==============================] - 0s 735us/step - loss: 1.2821 - accuracy: 0.7808 - val_loss: 1.2891 - val_accuracy: 0.5833\n",
      "Epoch 7/100\n",
      "219/219 [==============================] - 0s 723us/step - loss: 1.2426 - accuracy: 0.7763 - val_loss: 1.2537 - val_accuracy: 0.5417\n",
      "Epoch 8/100\n",
      "219/219 [==============================] - 0s 710us/step - loss: 1.1937 - accuracy: 0.7808 - val_loss: 1.2067 - val_accuracy: 0.5417\n",
      "Epoch 9/100\n",
      "219/219 [==============================] - 0s 717us/step - loss: 1.1271 - accuracy: 0.7763 - val_loss: 1.1410 - val_accuracy: 0.5417\n",
      "Epoch 10/100\n",
      "219/219 [==============================] - 0s 721us/step - loss: 1.0334 - accuracy: 0.7991 - val_loss: 1.0528 - val_accuracy: 0.6667\n",
      "Epoch 11/100\n",
      "219/219 [==============================] - 0s 712us/step - loss: 0.9222 - accuracy: 0.7717 - val_loss: 0.9419 - val_accuracy: 0.7083\n",
      "Epoch 12/100\n",
      "219/219 [==============================] - 0s 727us/step - loss: 0.7861 - accuracy: 0.7900 - val_loss: 0.8221 - val_accuracy: 0.7083\n",
      "Epoch 13/100\n",
      "219/219 [==============================] - 0s 714us/step - loss: 0.6456 - accuracy: 0.7900 - val_loss: 0.7158 - val_accuracy: 0.7500\n",
      "Epoch 14/100\n",
      "219/219 [==============================] - 0s 713us/step - loss: 0.5165 - accuracy: 0.8539 - val_loss: 0.6297 - val_accuracy: 0.7500\n",
      "Epoch 15/100\n",
      "219/219 [==============================] - 0s 711us/step - loss: 0.4148 - accuracy: 0.8721 - val_loss: 0.5568 - val_accuracy: 0.8333\n",
      "Epoch 16/100\n",
      "219/219 [==============================] - 0s 720us/step - loss: 0.3370 - accuracy: 0.8950 - val_loss: 0.5294 - val_accuracy: 0.8750\n",
      "Epoch 17/100\n",
      "219/219 [==============================] - 0s 723us/step - loss: 0.2530 - accuracy: 0.9498 - val_loss: 0.5163 - val_accuracy: 0.8333\n",
      "Epoch 18/100\n",
      "219/219 [==============================] - 0s 715us/step - loss: 0.2065 - accuracy: 0.9817 - val_loss: 0.5065 - val_accuracy: 0.8333\n",
      "Epoch 19/100\n",
      "219/219 [==============================] - 0s 722us/step - loss: 0.1705 - accuracy: 0.9772 - val_loss: 0.4656 - val_accuracy: 0.8333\n",
      "Epoch 20/100\n",
      "219/219 [==============================] - 0s 724us/step - loss: 0.1247 - accuracy: 0.9817 - val_loss: 0.4128 - val_accuracy: 0.8750\n",
      "Epoch 21/100\n",
      "219/219 [==============================] - 0s 720us/step - loss: 0.0875 - accuracy: 0.9954 - val_loss: 0.3676 - val_accuracy: 0.9167\n",
      "Epoch 22/100\n",
      "219/219 [==============================] - 0s 721us/step - loss: 0.0661 - accuracy: 0.9954 - val_loss: 0.3323 - val_accuracy: 0.9167\n",
      "Epoch 23/100\n",
      "219/219 [==============================] - 0s 715us/step - loss: 0.0509 - accuracy: 0.9954 - val_loss: 0.3135 - val_accuracy: 0.9167\n",
      "Epoch 24/100\n",
      "219/219 [==============================] - 0s 716us/step - loss: 0.0340 - accuracy: 1.0000 - val_loss: 0.3015 - val_accuracy: 0.9167\n",
      "Epoch 25/100\n",
      "219/219 [==============================] - 0s 722us/step - loss: 0.0297 - accuracy: 1.0000 - val_loss: 0.3051 - val_accuracy: 0.9167\n",
      "Epoch 26/100\n",
      "219/219 [==============================] - 0s 723us/step - loss: 0.0201 - accuracy: 1.0000 - val_loss: 0.3097 - val_accuracy: 0.9167\n",
      "Epoch 27/100\n",
      "219/219 [==============================] - 0s 707us/step - loss: 0.0207 - accuracy: 1.0000 - val_loss: 0.3128 - val_accuracy: 0.9167\n",
      "Train on 220 samples, validate on 23 samples\n",
      "Epoch 1/100\n",
      "220/220 [==============================] - 0s 716us/step - loss: 1.3851 - accuracy: 0.2636 - val_loss: 1.3783 - val_accuracy: 0.5217\n",
      "Epoch 2/100\n",
      "220/220 [==============================] - 0s 716us/step - loss: 1.3739 - accuracy: 0.4636 - val_loss: 1.3694 - val_accuracy: 0.5652\n",
      "Epoch 3/100\n",
      "220/220 [==============================] - 0s 711us/step - loss: 1.3574 - accuracy: 0.5909 - val_loss: 1.3583 - val_accuracy: 0.5217\n",
      "Epoch 4/100\n",
      "220/220 [==============================] - 0s 717us/step - loss: 1.3341 - accuracy: 0.7136 - val_loss: 1.3448 - val_accuracy: 0.5217\n",
      "Epoch 5/100\n",
      "220/220 [==============================] - 0s 711us/step - loss: 1.3070 - accuracy: 0.7545 - val_loss: 1.3293 - val_accuracy: 0.5217\n",
      "Epoch 6/100\n",
      "220/220 [==============================] - 0s 712us/step - loss: 1.2703 - accuracy: 0.7682 - val_loss: 1.3096 - val_accuracy: 0.5217\n",
      "Epoch 7/100\n",
      "220/220 [==============================] - 0s 703us/step - loss: 1.2292 - accuracy: 0.7818 - val_loss: 1.2827 - val_accuracy: 0.5217\n",
      "Epoch 8/100\n",
      "220/220 [==============================] - 0s 708us/step - loss: 1.1724 - accuracy: 0.7818 - val_loss: 1.2433 - val_accuracy: 0.5652\n",
      "Epoch 9/100\n",
      "220/220 [==============================] - 0s 718us/step - loss: 1.0892 - accuracy: 0.7909 - val_loss: 1.1851 - val_accuracy: 0.5652\n",
      "Epoch 10/100\n",
      "220/220 [==============================] - 0s 713us/step - loss: 0.9806 - accuracy: 0.8000 - val_loss: 1.0995 - val_accuracy: 0.6957\n",
      "Epoch 11/100\n",
      "220/220 [==============================] - 0s 708us/step - loss: 0.8500 - accuracy: 0.7682 - val_loss: 0.9929 - val_accuracy: 0.6522\n",
      "Epoch 12/100\n",
      "220/220 [==============================] - 0s 717us/step - loss: 0.7139 - accuracy: 0.7727 - val_loss: 0.9057 - val_accuracy: 0.6957\n",
      "Epoch 13/100\n",
      "220/220 [==============================] - 0s 714us/step - loss: 0.5907 - accuracy: 0.8227 - val_loss: 0.8824 - val_accuracy: 0.6522\n",
      "Epoch 14/100\n",
      "220/220 [==============================] - 0s 712us/step - loss: 0.4741 - accuracy: 0.8455 - val_loss: 0.7069 - val_accuracy: 0.6957\n",
      "Epoch 15/100\n",
      "220/220 [==============================] - 0s 712us/step - loss: 0.4022 - accuracy: 0.8273 - val_loss: 0.6158 - val_accuracy: 0.7391\n",
      "Epoch 16/100\n",
      "220/220 [==============================] - 0s 725us/step - loss: 0.3124 - accuracy: 0.9091 - val_loss: 0.6194 - val_accuracy: 0.7391\n",
      "Epoch 17/100\n",
      "220/220 [==============================] - 0s 722us/step - loss: 0.2656 - accuracy: 0.9545 - val_loss: 0.5635 - val_accuracy: 0.7391\n",
      "Epoch 18/100\n",
      "220/220 [==============================] - 0s 719us/step - loss: 0.2140 - accuracy: 0.9727 - val_loss: 0.5133 - val_accuracy: 0.7826\n",
      "Epoch 19/100\n",
      "220/220 [==============================] - 0s 711us/step - loss: 0.1736 - accuracy: 0.9773 - val_loss: 0.4696 - val_accuracy: 0.7826\n",
      "Epoch 20/100\n",
      "220/220 [==============================] - 0s 716us/step - loss: 0.1363 - accuracy: 0.9818 - val_loss: 0.4391 - val_accuracy: 0.8696\n",
      "Epoch 21/100\n",
      "220/220 [==============================] - 0s 719us/step - loss: 0.1037 - accuracy: 0.9909 - val_loss: 0.4171 - val_accuracy: 0.8696\n",
      "Epoch 22/100\n",
      "220/220 [==============================] - 0s 716us/step - loss: 0.0745 - accuracy: 1.0000 - val_loss: 0.4052 - val_accuracy: 0.8696\n",
      "Epoch 23/100\n",
      "220/220 [==============================] - 0s 716us/step - loss: 0.0578 - accuracy: 1.0000 - val_loss: 0.4042 - val_accuracy: 0.8696\n",
      "Epoch 24/100\n",
      "220/220 [==============================] - 0s 709us/step - loss: 0.0409 - accuracy: 1.0000 - val_loss: 0.4019 - val_accuracy: 0.8696\n",
      "Epoch 25/100\n",
      "220/220 [==============================] - 0s 713us/step - loss: 0.0302 - accuracy: 1.0000 - val_loss: 0.4026 - val_accuracy: 0.8696\n",
      "Epoch 26/100\n",
      "220/220 [==============================] - 0s 713us/step - loss: 0.0209 - accuracy: 1.0000 - val_loss: 0.4133 - val_accuracy: 0.8261\n",
      "Epoch 27/100\n",
      "220/220 [==============================] - 0s 715us/step - loss: 0.0188 - accuracy: 1.0000 - val_loss: 0.4200 - val_accuracy: 0.8261\n",
      "Train on 218 samples, validate on 25 samples\n",
      "Epoch 1/100\n",
      "218/218 [==============================] - 1s 3ms/step - loss: 1.3810 - accuracy: 0.2844 - val_loss: 1.3671 - val_accuracy: 0.3200\n",
      "Epoch 2/100\n",
      "218/218 [==============================] - 0s 776us/step - loss: 1.3526 - accuracy: 0.4083 - val_loss: 1.3528 - val_accuracy: 0.5200\n",
      "Epoch 3/100\n",
      "218/218 [==============================] - 0s 758us/step - loss: 1.3273 - accuracy: 0.4679 - val_loss: 1.3398 - val_accuracy: 0.4000\n",
      "Epoch 4/100\n",
      "218/218 [==============================] - 0s 747us/step - loss: 1.3044 - accuracy: 0.4404 - val_loss: 1.3203 - val_accuracy: 0.4000\n",
      "Epoch 5/100\n",
      "218/218 [==============================] - 0s 747us/step - loss: 1.2502 - accuracy: 0.4908 - val_loss: 1.2802 - val_accuracy: 0.5200\n",
      "Epoch 6/100\n",
      "218/218 [==============================] - 0s 748us/step - loss: 1.1697 - accuracy: 0.6193 - val_loss: 1.2188 - val_accuracy: 0.5200\n",
      "Epoch 7/100\n",
      "218/218 [==============================] - 0s 743us/step - loss: 1.0510 - accuracy: 0.7752 - val_loss: 1.1411 - val_accuracy: 0.5200\n",
      "Epoch 8/100\n",
      "218/218 [==============================] - 0s 740us/step - loss: 0.9154 - accuracy: 0.8119 - val_loss: 1.0608 - val_accuracy: 0.6000\n",
      "Epoch 9/100\n",
      "218/218 [==============================] - 0s 747us/step - loss: 0.7691 - accuracy: 0.8028 - val_loss: 1.0136 - val_accuracy: 0.6000\n",
      "Epoch 10/100\n",
      "218/218 [==============================] - 0s 754us/step - loss: 0.6018 - accuracy: 0.8761 - val_loss: 0.9512 - val_accuracy: 0.6800\n",
      "Epoch 11/100\n",
      "218/218 [==============================] - 0s 741us/step - loss: 0.4635 - accuracy: 0.8991 - val_loss: 0.8967 - val_accuracy: 0.7200\n",
      "Epoch 12/100\n",
      "218/218 [==============================] - 0s 750us/step - loss: 0.3898 - accuracy: 0.8945 - val_loss: 0.8658 - val_accuracy: 0.6800\n",
      "Epoch 13/100\n",
      "218/218 [==============================] - 0s 743us/step - loss: 0.2838 - accuracy: 0.9312 - val_loss: 0.8329 - val_accuracy: 0.8000\n",
      "Epoch 14/100\n",
      "218/218 [==============================] - 0s 734us/step - loss: 0.2130 - accuracy: 0.9587 - val_loss: 0.7982 - val_accuracy: 0.8000\n",
      "Epoch 15/100\n",
      "218/218 [==============================] - 0s 751us/step - loss: 0.1633 - accuracy: 0.9817 - val_loss: 0.7836 - val_accuracy: 0.8000\n",
      "Epoch 16/100\n",
      "218/218 [==============================] - 0s 743us/step - loss: 0.1191 - accuracy: 0.9817 - val_loss: 0.7952 - val_accuracy: 0.7600\n",
      "Epoch 17/100\n",
      "218/218 [==============================] - 0s 755us/step - loss: 0.0967 - accuracy: 0.9817 - val_loss: 0.7802 - val_accuracy: 0.7600\n",
      "Epoch 18/100\n",
      "218/218 [==============================] - 0s 751us/step - loss: 0.0749 - accuracy: 0.9862 - val_loss: 0.7455 - val_accuracy: 0.8400\n",
      "Epoch 19/100\n",
      "218/218 [==============================] - 0s 749us/step - loss: 0.0531 - accuracy: 0.9862 - val_loss: 0.7352 - val_accuracy: 0.8400\n",
      "Epoch 20/100\n",
      "218/218 [==============================] - 0s 751us/step - loss: 0.0416 - accuracy: 0.9908 - val_loss: 0.7379 - val_accuracy: 0.8400\n",
      "Epoch 21/100\n",
      "218/218 [==============================] - 0s 753us/step - loss: 0.0363 - accuracy: 0.9954 - val_loss: 0.7625 - val_accuracy: 0.7600\n",
      "Epoch 22/100\n",
      "218/218 [==============================] - 0s 750us/step - loss: 0.0319 - accuracy: 0.9954 - val_loss: 0.7840 - val_accuracy: 0.7600\n",
      "Train on 218 samples, validate on 25 samples\n",
      "Epoch 1/100\n",
      "218/218 [==============================] - 0s 751us/step - loss: 1.3846 - accuracy: 0.2706 - val_loss: 1.3785 - val_accuracy: 0.3600\n",
      "Epoch 2/100\n",
      "218/218 [==============================] - 0s 757us/step - loss: 1.3744 - accuracy: 0.3899 - val_loss: 1.3658 - val_accuracy: 0.5200\n",
      "Epoch 3/100\n",
      "218/218 [==============================] - 0s 750us/step - loss: 1.3646 - accuracy: 0.4266 - val_loss: 1.3519 - val_accuracy: 0.3600\n",
      "Epoch 4/100\n",
      "218/218 [==============================] - 0s 745us/step - loss: 1.3489 - accuracy: 0.3991 - val_loss: 1.3427 - val_accuracy: 0.4000\n",
      "Epoch 5/100\n",
      "218/218 [==============================] - 0s 742us/step - loss: 1.3480 - accuracy: 0.4312 - val_loss: 1.3357 - val_accuracy: 0.4800\n",
      "Epoch 6/100\n",
      "218/218 [==============================] - 0s 753us/step - loss: 1.3370 - accuracy: 0.4450 - val_loss: 1.3292 - val_accuracy: 0.3600\n",
      "Epoch 7/100\n",
      "218/218 [==============================] - 0s 742us/step - loss: 1.3281 - accuracy: 0.4128 - val_loss: 1.3216 - val_accuracy: 0.3600\n",
      "Epoch 8/100\n",
      "218/218 [==============================] - 0s 730us/step - loss: 1.3123 - accuracy: 0.4450 - val_loss: 1.3105 - val_accuracy: 0.3600\n",
      "Epoch 9/100\n",
      "218/218 [==============================] - 0s 749us/step - loss: 1.2973 - accuracy: 0.4495 - val_loss: 1.2938 - val_accuracy: 0.4000\n",
      "Epoch 10/100\n",
      "218/218 [==============================] - 0s 745us/step - loss: 1.2681 - accuracy: 0.5092 - val_loss: 1.2697 - val_accuracy: 0.4400\n",
      "Epoch 11/100\n",
      "218/218 [==============================] - 0s 743us/step - loss: 1.2218 - accuracy: 0.6330 - val_loss: 1.2348 - val_accuracy: 0.6400\n",
      "Epoch 12/100\n",
      "218/218 [==============================] - 0s 741us/step - loss: 1.1588 - accuracy: 0.7202 - val_loss: 1.1856 - val_accuracy: 0.6800\n",
      "Epoch 13/100\n",
      "218/218 [==============================] - 0s 745us/step - loss: 1.0674 - accuracy: 0.7936 - val_loss: 1.1267 - val_accuracy: 0.7200\n",
      "Epoch 14/100\n",
      "218/218 [==============================] - 0s 751us/step - loss: 0.9605 - accuracy: 0.7890 - val_loss: 1.0594 - val_accuracy: 0.7600\n",
      "Epoch 15/100\n",
      "218/218 [==============================] - 0s 752us/step - loss: 0.8572 - accuracy: 0.8119 - val_loss: 0.9735 - val_accuracy: 0.8000\n",
      "Epoch 16/100\n",
      "218/218 [==============================] - 0s 745us/step - loss: 0.7437 - accuracy: 0.8486 - val_loss: 0.8828 - val_accuracy: 0.8400\n",
      "Epoch 17/100\n",
      "218/218 [==============================] - 0s 749us/step - loss: 0.6315 - accuracy: 0.8899 - val_loss: 0.7835 - val_accuracy: 0.7600\n",
      "Epoch 18/100\n",
      "218/218 [==============================] - 0s 756us/step - loss: 0.5061 - accuracy: 0.9083 - val_loss: 0.6787 - val_accuracy: 0.7600\n",
      "Epoch 19/100\n",
      "218/218 [==============================] - 0s 747us/step - loss: 0.3868 - accuracy: 0.9404 - val_loss: 0.5677 - val_accuracy: 0.8400\n",
      "Epoch 20/100\n",
      "218/218 [==============================] - 0s 745us/step - loss: 0.2774 - accuracy: 0.9495 - val_loss: 0.4806 - val_accuracy: 0.7200\n",
      "Epoch 21/100\n",
      "218/218 [==============================] - 0s 745us/step - loss: 0.2249 - accuracy: 0.9495 - val_loss: 0.4150 - val_accuracy: 0.8000\n",
      "Epoch 22/100\n",
      "218/218 [==============================] - 0s 740us/step - loss: 0.1568 - accuracy: 0.9725 - val_loss: 0.3773 - val_accuracy: 0.8000\n",
      "Epoch 23/100\n",
      "218/218 [==============================] - 0s 752us/step - loss: 0.1112 - accuracy: 0.9771 - val_loss: 0.3565 - val_accuracy: 0.8000\n",
      "Epoch 24/100\n",
      "218/218 [==============================] - 0s 761us/step - loss: 0.0773 - accuracy: 0.9817 - val_loss: 0.3363 - val_accuracy: 0.9200\n",
      "Epoch 25/100\n",
      "218/218 [==============================] - 0s 751us/step - loss: 0.0658 - accuracy: 0.9862 - val_loss: 0.3259 - val_accuracy: 0.9600\n",
      "Epoch 26/100\n",
      "218/218 [==============================] - 0s 751us/step - loss: 0.0439 - accuracy: 0.9954 - val_loss: 0.3349 - val_accuracy: 0.8400\n",
      "Epoch 27/100\n",
      "218/218 [==============================] - 0s 755us/step - loss: 0.0295 - accuracy: 1.0000 - val_loss: 0.3597 - val_accuracy: 0.8000\n",
      "Epoch 28/100\n",
      "218/218 [==============================] - 0s 746us/step - loss: 0.0268 - accuracy: 0.9954 - val_loss: 0.3905 - val_accuracy: 0.8000\n",
      "Train on 218 samples, validate on 25 samples\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "218/218 [==============================] - 0s 736us/step - loss: 1.3871 - accuracy: 0.2431 - val_loss: 1.3769 - val_accuracy: 0.2800\n",
      "Epoch 2/100\n",
      "218/218 [==============================] - 0s 759us/step - loss: 1.3786 - accuracy: 0.3440 - val_loss: 1.3641 - val_accuracy: 0.5200\n",
      "Epoch 3/100\n",
      "218/218 [==============================] - 0s 751us/step - loss: 1.3662 - accuracy: 0.3807 - val_loss: 1.3456 - val_accuracy: 0.3200\n",
      "Epoch 4/100\n",
      "218/218 [==============================] - 0s 747us/step - loss: 1.3594 - accuracy: 0.3807 - val_loss: 1.3298 - val_accuracy: 0.3200\n",
      "Epoch 5/100\n",
      "218/218 [==============================] - 0s 745us/step - loss: 1.3499 - accuracy: 0.4220 - val_loss: 1.3194 - val_accuracy: 0.3200\n",
      "Epoch 6/100\n",
      "218/218 [==============================] - 0s 751us/step - loss: 1.3389 - accuracy: 0.4495 - val_loss: 1.3115 - val_accuracy: 0.3200\n",
      "Epoch 7/100\n",
      "218/218 [==============================] - 0s 753us/step - loss: 1.3319 - accuracy: 0.4495 - val_loss: 1.3042 - val_accuracy: 0.3600\n",
      "Epoch 8/100\n",
      "218/218 [==============================] - 0s 743us/step - loss: 1.3169 - accuracy: 0.4587 - val_loss: 1.2967 - val_accuracy: 0.4000\n",
      "Epoch 9/100\n",
      "218/218 [==============================] - 0s 752us/step - loss: 1.2987 - accuracy: 0.4862 - val_loss: 1.2878 - val_accuracy: 0.4800\n",
      "Epoch 10/100\n",
      "218/218 [==============================] - 0s 744us/step - loss: 1.2717 - accuracy: 0.5688 - val_loss: 1.2704 - val_accuracy: 0.5200\n",
      "Epoch 11/100\n",
      "218/218 [==============================] - 0s 754us/step - loss: 1.2315 - accuracy: 0.6239 - val_loss: 1.2385 - val_accuracy: 0.5200\n",
      "Epoch 12/100\n",
      "218/218 [==============================] - 0s 750us/step - loss: 1.1673 - accuracy: 0.6743 - val_loss: 1.1905 - val_accuracy: 0.6000\n",
      "Epoch 13/100\n",
      "218/218 [==============================] - 0s 745us/step - loss: 1.0853 - accuracy: 0.7339 - val_loss: 1.1276 - val_accuracy: 0.7200\n",
      "Epoch 14/100\n",
      "218/218 [==============================] - 0s 749us/step - loss: 0.9762 - accuracy: 0.7798 - val_loss: 1.0489 - val_accuracy: 0.6800\n",
      "Epoch 15/100\n",
      "218/218 [==============================] - 0s 758us/step - loss: 0.8674 - accuracy: 0.8073 - val_loss: 0.9584 - val_accuracy: 0.7600\n",
      "Epoch 16/100\n",
      "218/218 [==============================] - 0s 759us/step - loss: 0.7548 - accuracy: 0.8349 - val_loss: 0.8606 - val_accuracy: 0.7600\n",
      "Epoch 17/100\n",
      "218/218 [==============================] - 0s 754us/step - loss: 0.6402 - accuracy: 0.8624 - val_loss: 0.7671 - val_accuracy: 0.7600\n",
      "Epoch 18/100\n",
      "218/218 [==============================] - 0s 751us/step - loss: 0.5008 - accuracy: 0.8945 - val_loss: 0.6647 - val_accuracy: 0.7600\n",
      "Epoch 19/100\n",
      "218/218 [==============================] - 0s 748us/step - loss: 0.3909 - accuracy: 0.9174 - val_loss: 0.5535 - val_accuracy: 0.7600\n",
      "Epoch 20/100\n",
      "218/218 [==============================] - 0s 758us/step - loss: 0.2825 - accuracy: 0.9312 - val_loss: 0.4989 - val_accuracy: 0.8000\n",
      "Epoch 21/100\n",
      "218/218 [==============================] - 0s 755us/step - loss: 0.2267 - accuracy: 0.9404 - val_loss: 0.4531 - val_accuracy: 0.8000\n",
      "Epoch 22/100\n",
      "218/218 [==============================] - 0s 749us/step - loss: 0.1449 - accuracy: 0.9771 - val_loss: 0.3676 - val_accuracy: 0.8800\n",
      "Epoch 23/100\n",
      "218/218 [==============================] - 0s 737us/step - loss: 0.1113 - accuracy: 0.9679 - val_loss: 0.3100 - val_accuracy: 0.9200\n",
      "Epoch 24/100\n",
      "218/218 [==============================] - 0s 733us/step - loss: 0.0824 - accuracy: 0.9817 - val_loss: 0.2617 - val_accuracy: 0.9200\n",
      "Epoch 25/100\n",
      "218/218 [==============================] - 0s 743us/step - loss: 0.0594 - accuracy: 0.9862 - val_loss: 0.2560 - val_accuracy: 0.9200\n",
      "Epoch 26/100\n",
      "218/218 [==============================] - 0s 745us/step - loss: 0.0548 - accuracy: 0.9862 - val_loss: 0.2427 - val_accuracy: 0.8800\n",
      "Epoch 27/100\n",
      "218/218 [==============================] - 0s 745us/step - loss: 0.0335 - accuracy: 0.9954 - val_loss: 0.2499 - val_accuracy: 0.8800\n",
      "Epoch 28/100\n",
      "218/218 [==============================] - 0s 751us/step - loss: 0.0293 - accuracy: 0.9954 - val_loss: 0.3148 - val_accuracy: 0.8800\n",
      "Epoch 29/100\n",
      "218/218 [==============================] - 0s 749us/step - loss: 0.0271 - accuracy: 0.9954 - val_loss: 0.3605 - val_accuracy: 0.8800\n",
      "Train on 218 samples, validate on 25 samples\n",
      "Epoch 1/100\n",
      "218/218 [==============================] - 0s 754us/step - loss: 1.3864 - accuracy: 0.2385 - val_loss: 1.3800 - val_accuracy: 0.4000\n",
      "Epoch 2/100\n",
      "218/218 [==============================] - 0s 755us/step - loss: 1.3786 - accuracy: 0.3670 - val_loss: 1.3711 - val_accuracy: 0.4000\n",
      "Epoch 3/100\n",
      "218/218 [==============================] - 0s 751us/step - loss: 1.3649 - accuracy: 0.3761 - val_loss: 1.3615 - val_accuracy: 0.4000\n",
      "Epoch 4/100\n",
      "218/218 [==============================] - 0s 761us/step - loss: 1.3565 - accuracy: 0.4083 - val_loss: 1.3545 - val_accuracy: 0.4000\n",
      "Epoch 5/100\n",
      "218/218 [==============================] - 0s 750us/step - loss: 1.3443 - accuracy: 0.4266 - val_loss: 1.3491 - val_accuracy: 0.4000\n",
      "Epoch 6/100\n",
      "218/218 [==============================] - 0s 742us/step - loss: 1.3408 - accuracy: 0.3991 - val_loss: 1.3442 - val_accuracy: 0.4000\n",
      "Epoch 7/100\n",
      "218/218 [==============================] - 0s 747us/step - loss: 1.3320 - accuracy: 0.4450 - val_loss: 1.3378 - val_accuracy: 0.3600\n",
      "Epoch 8/100\n",
      "218/218 [==============================] - 0s 748us/step - loss: 1.3198 - accuracy: 0.4725 - val_loss: 1.3282 - val_accuracy: 0.3600\n",
      "Epoch 9/100\n",
      "218/218 [==============================] - 0s 751us/step - loss: 1.2999 - accuracy: 0.5046 - val_loss: 1.3145 - val_accuracy: 0.3600\n",
      "Epoch 10/100\n",
      "218/218 [==============================] - 0s 761us/step - loss: 1.2784 - accuracy: 0.5550 - val_loss: 1.2923 - val_accuracy: 0.4400\n",
      "Epoch 11/100\n",
      "218/218 [==============================] - 0s 748us/step - loss: 1.2450 - accuracy: 0.5917 - val_loss: 1.2554 - val_accuracy: 0.4400\n",
      "Epoch 12/100\n",
      "218/218 [==============================] - 0s 747us/step - loss: 1.1911 - accuracy: 0.6651 - val_loss: 1.1998 - val_accuracy: 0.4800\n",
      "Epoch 13/100\n",
      "218/218 [==============================] - 0s 743us/step - loss: 1.1175 - accuracy: 0.6927 - val_loss: 1.1294 - val_accuracy: 0.6000\n",
      "Epoch 14/100\n",
      "218/218 [==============================] - 0s 744us/step - loss: 1.0284 - accuracy: 0.7385 - val_loss: 1.0505 - val_accuracy: 0.6000\n",
      "Epoch 15/100\n",
      "218/218 [==============================] - 0s 752us/step - loss: 0.9264 - accuracy: 0.7385 - val_loss: 0.9619 - val_accuracy: 0.6400\n",
      "Epoch 16/100\n",
      "218/218 [==============================] - 0s 731us/step - loss: 0.8112 - accuracy: 0.7936 - val_loss: 0.8646 - val_accuracy: 0.6400\n",
      "Epoch 17/100\n",
      "218/218 [==============================] - 0s 743us/step - loss: 0.7083 - accuracy: 0.8119 - val_loss: 0.7631 - val_accuracy: 0.7600\n",
      "Epoch 18/100\n",
      "218/218 [==============================] - 0s 743us/step - loss: 0.5798 - accuracy: 0.8670 - val_loss: 0.6439 - val_accuracy: 0.8800\n",
      "Epoch 19/100\n",
      "218/218 [==============================] - 0s 745us/step - loss: 0.4509 - accuracy: 0.9128 - val_loss: 0.5179 - val_accuracy: 0.8800\n",
      "Epoch 20/100\n",
      "218/218 [==============================] - 0s 750us/step - loss: 0.3403 - accuracy: 0.9450 - val_loss: 0.4166 - val_accuracy: 0.8800\n",
      "Epoch 21/100\n",
      "218/218 [==============================] - 0s 751us/step - loss: 0.2421 - accuracy: 0.9495 - val_loss: 0.3279 - val_accuracy: 0.9200\n",
      "Epoch 22/100\n",
      "218/218 [==============================] - 0s 744us/step - loss: 0.1728 - accuracy: 0.9679 - val_loss: 0.3401 - val_accuracy: 0.8400\n",
      "Epoch 23/100\n",
      "218/218 [==============================] - 0s 746us/step - loss: 0.1213 - accuracy: 0.9817 - val_loss: 0.2536 - val_accuracy: 0.8800\n",
      "Epoch 24/100\n",
      "218/218 [==============================] - 0s 757us/step - loss: 0.0911 - accuracy: 0.9817 - val_loss: 0.2570 - val_accuracy: 0.8800\n",
      "Epoch 25/100\n",
      "218/218 [==============================] - 0s 752us/step - loss: 0.0670 - accuracy: 0.9817 - val_loss: 0.2926 - val_accuracy: 0.8800\n",
      "Epoch 26/100\n",
      "218/218 [==============================] - 0s 741us/step - loss: 0.0457 - accuracy: 0.9954 - val_loss: 0.3216 - val_accuracy: 0.8400\n",
      "Train on 219 samples, validate on 24 samples\n",
      "Epoch 1/100\n",
      "219/219 [==============================] - 0s 751us/step - loss: 1.3825 - accuracy: 0.3105 - val_loss: 1.3788 - val_accuracy: 0.3750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "219/219 [==============================] - 0s 752us/step - loss: 1.3768 - accuracy: 0.3516 - val_loss: 1.3668 - val_accuracy: 0.3750\n",
      "Epoch 3/100\n",
      "219/219 [==============================] - 0s 744us/step - loss: 1.3613 - accuracy: 0.4155 - val_loss: 1.3546 - val_accuracy: 0.3750\n",
      "Epoch 4/100\n",
      "219/219 [==============================] - 0s 750us/step - loss: 1.3540 - accuracy: 0.3973 - val_loss: 1.3435 - val_accuracy: 0.3750\n",
      "Epoch 5/100\n",
      "219/219 [==============================] - 0s 742us/step - loss: 1.3440 - accuracy: 0.4064 - val_loss: 1.3335 - val_accuracy: 0.3750\n",
      "Epoch 6/100\n",
      "219/219 [==============================] - 0s 761us/step - loss: 1.3323 - accuracy: 0.4429 - val_loss: 1.3238 - val_accuracy: 0.3750\n",
      "Epoch 7/100\n",
      "219/219 [==============================] - 0s 741us/step - loss: 1.3205 - accuracy: 0.4521 - val_loss: 1.3119 - val_accuracy: 0.4583\n",
      "Epoch 8/100\n",
      "219/219 [==============================] - 0s 749us/step - loss: 1.3045 - accuracy: 0.5160 - val_loss: 1.2948 - val_accuracy: 0.4583\n",
      "Epoch 9/100\n",
      "219/219 [==============================] - 0s 753us/step - loss: 1.2843 - accuracy: 0.5479 - val_loss: 1.2712 - val_accuracy: 0.5417\n",
      "Epoch 10/100\n",
      "219/219 [==============================] - 0s 740us/step - loss: 1.2467 - accuracy: 0.6027 - val_loss: 1.2372 - val_accuracy: 0.6250\n",
      "Epoch 11/100\n",
      "219/219 [==============================] - 0s 754us/step - loss: 1.1911 - accuracy: 0.6712 - val_loss: 1.1903 - val_accuracy: 0.6667\n",
      "Epoch 12/100\n",
      "219/219 [==============================] - 0s 752us/step - loss: 1.1117 - accuracy: 0.7123 - val_loss: 1.1276 - val_accuracy: 0.7083\n",
      "Epoch 13/100\n",
      "219/219 [==============================] - 0s 747us/step - loss: 1.0158 - accuracy: 0.7717 - val_loss: 1.0479 - val_accuracy: 0.6667\n",
      "Epoch 14/100\n",
      "219/219 [==============================] - 0s 748us/step - loss: 0.8918 - accuracy: 0.7854 - val_loss: 0.9500 - val_accuracy: 0.6250\n",
      "Epoch 15/100\n",
      "219/219 [==============================] - 0s 739us/step - loss: 0.7564 - accuracy: 0.8311 - val_loss: 0.8393 - val_accuracy: 0.7083\n",
      "Epoch 16/100\n",
      "219/219 [==============================] - 0s 742us/step - loss: 0.6084 - accuracy: 0.8676 - val_loss: 0.7294 - val_accuracy: 0.7917\n",
      "Epoch 17/100\n",
      "219/219 [==============================] - 0s 737us/step - loss: 0.4649 - accuracy: 0.9041 - val_loss: 0.6326 - val_accuracy: 0.7500\n",
      "Epoch 18/100\n",
      "219/219 [==============================] - 0s 744us/step - loss: 0.3328 - accuracy: 0.9224 - val_loss: 0.5508 - val_accuracy: 0.7500\n",
      "Epoch 19/100\n",
      "219/219 [==============================] - 0s 736us/step - loss: 0.2306 - accuracy: 0.9543 - val_loss: 0.5157 - val_accuracy: 0.7917\n",
      "Epoch 20/100\n",
      "219/219 [==============================] - 0s 745us/step - loss: 0.1399 - accuracy: 0.9817 - val_loss: 0.5483 - val_accuracy: 0.7917\n",
      "Epoch 21/100\n",
      "219/219 [==============================] - 0s 751us/step - loss: 0.1053 - accuracy: 0.9772 - val_loss: 0.5820 - val_accuracy: 0.7917\n",
      "Epoch 22/100\n",
      "219/219 [==============================] - 0s 740us/step - loss: 0.0740 - accuracy: 0.9817 - val_loss: 0.5550 - val_accuracy: 0.8333\n",
      "Train on 219 samples, validate on 24 samples\n",
      "Epoch 1/100\n",
      "219/219 [==============================] - 0s 749us/step - loss: 1.3833 - accuracy: 0.3242 - val_loss: 1.3781 - val_accuracy: 0.3750\n",
      "Epoch 2/100\n",
      "219/219 [==============================] - 0s 740us/step - loss: 1.3748 - accuracy: 0.3927 - val_loss: 1.3657 - val_accuracy: 0.3333\n",
      "Epoch 3/100\n",
      "219/219 [==============================] - 0s 739us/step - loss: 1.3593 - accuracy: 0.4018 - val_loss: 1.3538 - val_accuracy: 0.3333\n",
      "Epoch 4/100\n",
      "219/219 [==============================] - 0s 743us/step - loss: 1.3481 - accuracy: 0.4110 - val_loss: 1.3450 - val_accuracy: 0.3333\n",
      "Epoch 5/100\n",
      "219/219 [==============================] - 0s 749us/step - loss: 1.3325 - accuracy: 0.4064 - val_loss: 1.3370 - val_accuracy: 0.3333\n",
      "Epoch 6/100\n",
      "219/219 [==============================] - 0s 743us/step - loss: 1.3253 - accuracy: 0.4064 - val_loss: 1.3275 - val_accuracy: 0.3333\n",
      "Epoch 7/100\n",
      "219/219 [==============================] - 0s 745us/step - loss: 1.3070 - accuracy: 0.4384 - val_loss: 1.3138 - val_accuracy: 0.3333\n",
      "Epoch 8/100\n",
      "219/219 [==============================] - 0s 739us/step - loss: 1.2813 - accuracy: 0.4566 - val_loss: 1.2917 - val_accuracy: 0.3333\n",
      "Epoch 9/100\n",
      "219/219 [==============================] - 0s 742us/step - loss: 1.2475 - accuracy: 0.5251 - val_loss: 1.2560 - val_accuracy: 0.3750\n",
      "Epoch 10/100\n",
      "219/219 [==============================] - 0s 743us/step - loss: 1.1964 - accuracy: 0.6164 - val_loss: 1.2040 - val_accuracy: 0.4583\n",
      "Epoch 11/100\n",
      "219/219 [==============================] - 0s 740us/step - loss: 1.1116 - accuracy: 0.7534 - val_loss: 1.1318 - val_accuracy: 0.6667\n",
      "Epoch 12/100\n",
      "219/219 [==============================] - 0s 743us/step - loss: 1.0054 - accuracy: 0.8128 - val_loss: 1.0230 - val_accuracy: 0.7917\n",
      "Epoch 13/100\n",
      "219/219 [==============================] - 0s 745us/step - loss: 0.8634 - accuracy: 0.8128 - val_loss: 0.8754 - val_accuracy: 0.7917\n",
      "Epoch 14/100\n",
      "219/219 [==============================] - 0s 741us/step - loss: 0.6902 - accuracy: 0.8676 - val_loss: 0.7118 - val_accuracy: 0.7917\n",
      "Epoch 15/100\n",
      "219/219 [==============================] - 0s 742us/step - loss: 0.5063 - accuracy: 0.8995 - val_loss: 0.5930 - val_accuracy: 0.8333\n",
      "Epoch 16/100\n",
      "219/219 [==============================] - 0s 741us/step - loss: 0.3617 - accuracy: 0.9224 - val_loss: 0.5083 - val_accuracy: 0.8750\n",
      "Epoch 17/100\n",
      "219/219 [==============================] - 0s 737us/step - loss: 0.2518 - accuracy: 0.9589 - val_loss: 0.4774 - val_accuracy: 0.8750\n",
      "Epoch 18/100\n",
      "219/219 [==============================] - 0s 755us/step - loss: 0.1703 - accuracy: 0.9680 - val_loss: 0.4940 - val_accuracy: 0.8333\n",
      "Epoch 19/100\n",
      "219/219 [==============================] - 0s 743us/step - loss: 0.1193 - accuracy: 0.9726 - val_loss: 0.4423 - val_accuracy: 0.8333\n",
      "Epoch 20/100\n",
      "219/219 [==============================] - 0s 741us/step - loss: 0.0793 - accuracy: 0.9817 - val_loss: 0.4477 - val_accuracy: 0.7917\n",
      "Epoch 21/100\n",
      "219/219 [==============================] - 0s 751us/step - loss: 0.0717 - accuracy: 0.9772 - val_loss: 0.4692 - val_accuracy: 0.7917\n",
      "Epoch 22/100\n",
      "219/219 [==============================] - 0s 751us/step - loss: 0.0482 - accuracy: 0.9863 - val_loss: 0.4852 - val_accuracy: 0.8333\n",
      "Train on 219 samples, validate on 24 samples\n",
      "Epoch 1/100\n",
      "219/219 [==============================] - 0s 748us/step - loss: 1.3861 - accuracy: 0.2877 - val_loss: 1.3788 - val_accuracy: 0.5417\n",
      "Epoch 2/100\n",
      "219/219 [==============================] - 0s 726us/step - loss: 1.3816 - accuracy: 0.3014 - val_loss: 1.3683 - val_accuracy: 0.5000\n",
      "Epoch 3/100\n",
      "219/219 [==============================] - 0s 741us/step - loss: 1.3641 - accuracy: 0.4110 - val_loss: 1.3568 - val_accuracy: 0.4583\n",
      "Epoch 4/100\n",
      "219/219 [==============================] - 0s 747us/step - loss: 1.3533 - accuracy: 0.3927 - val_loss: 1.3463 - val_accuracy: 0.5000\n",
      "Epoch 5/100\n",
      "219/219 [==============================] - 0s 739us/step - loss: 1.3419 - accuracy: 0.4338 - val_loss: 1.3375 - val_accuracy: 0.5000\n",
      "Epoch 6/100\n",
      "219/219 [==============================] - 0s 750us/step - loss: 1.3324 - accuracy: 0.4155 - val_loss: 1.3298 - val_accuracy: 0.4583\n",
      "Epoch 7/100\n",
      "219/219 [==============================] - 0s 748us/step - loss: 1.3216 - accuracy: 0.4292 - val_loss: 1.3227 - val_accuracy: 0.4583\n",
      "Epoch 8/100\n",
      "219/219 [==============================] - 0s 803us/step - loss: 1.3081 - accuracy: 0.4475 - val_loss: 1.3128 - val_accuracy: 0.4583\n",
      "Epoch 9/100\n",
      "219/219 [==============================] - 0s 745us/step - loss: 1.2874 - accuracy: 0.4521 - val_loss: 1.2977 - val_accuracy: 0.4583\n",
      "Epoch 10/100\n",
      "219/219 [==============================] - 0s 745us/step - loss: 1.2506 - accuracy: 0.5068 - val_loss: 1.2754 - val_accuracy: 0.4583\n",
      "Epoch 11/100\n",
      "219/219 [==============================] - 0s 748us/step - loss: 1.1984 - accuracy: 0.5753 - val_loss: 1.2424 - val_accuracy: 0.5833\n",
      "Epoch 12/100\n",
      "219/219 [==============================] - 0s 743us/step - loss: 1.1234 - accuracy: 0.6667 - val_loss: 1.1911 - val_accuracy: 0.5417\n",
      "Epoch 13/100\n",
      "219/219 [==============================] - 0s 745us/step - loss: 1.0343 - accuracy: 0.7306 - val_loss: 1.1202 - val_accuracy: 0.6667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/100\n",
      "219/219 [==============================] - 0s 737us/step - loss: 0.9253 - accuracy: 0.7763 - val_loss: 1.0372 - val_accuracy: 0.7083\n",
      "Epoch 15/100\n",
      "219/219 [==============================] - 0s 744us/step - loss: 0.8113 - accuracy: 0.8082 - val_loss: 0.9570 - val_accuracy: 0.7083\n",
      "Epoch 16/100\n",
      "219/219 [==============================] - 0s 744us/step - loss: 0.6971 - accuracy: 0.8447 - val_loss: 0.8851 - val_accuracy: 0.7500\n",
      "Epoch 17/100\n",
      "219/219 [==============================] - 0s 745us/step - loss: 0.5561 - accuracy: 0.9087 - val_loss: 0.8197 - val_accuracy: 0.6667\n",
      "Epoch 18/100\n",
      "219/219 [==============================] - 0s 744us/step - loss: 0.4313 - accuracy: 0.9361 - val_loss: 0.7416 - val_accuracy: 0.7083\n",
      "Epoch 19/100\n",
      "219/219 [==============================] - 0s 739us/step - loss: 0.3120 - accuracy: 0.9589 - val_loss: 0.6389 - val_accuracy: 0.8333\n",
      "Epoch 20/100\n",
      "219/219 [==============================] - 0s 751us/step - loss: 0.2252 - accuracy: 0.9635 - val_loss: 0.5654 - val_accuracy: 0.8750\n",
      "Epoch 21/100\n",
      "219/219 [==============================] - 0s 740us/step - loss: 0.1518 - accuracy: 0.9772 - val_loss: 0.5632 - val_accuracy: 0.7917\n",
      "Epoch 22/100\n",
      "219/219 [==============================] - 0s 736us/step - loss: 0.1088 - accuracy: 0.9817 - val_loss: 0.4247 - val_accuracy: 0.9167\n",
      "Epoch 23/100\n",
      "219/219 [==============================] - 0s 746us/step - loss: 0.0777 - accuracy: 0.9863 - val_loss: 0.4416 - val_accuracy: 0.8333\n",
      "Epoch 24/100\n",
      "219/219 [==============================] - 0s 743us/step - loss: 0.0549 - accuracy: 0.9909 - val_loss: 0.4682 - val_accuracy: 0.8333\n",
      "Epoch 25/100\n",
      "219/219 [==============================] - 0s 744us/step - loss: 0.0495 - accuracy: 0.9863 - val_loss: 0.4494 - val_accuracy: 0.8333\n",
      "Train on 219 samples, validate on 24 samples\n",
      "Epoch 1/100\n",
      "219/219 [==============================] - 0s 739us/step - loss: 1.3870 - accuracy: 0.2329 - val_loss: 1.3849 - val_accuracy: 0.4583\n",
      "Epoch 2/100\n",
      "219/219 [==============================] - 0s 745us/step - loss: 1.3735 - accuracy: 0.3744 - val_loss: 1.3835 - val_accuracy: 0.5000\n",
      "Epoch 3/100\n",
      "219/219 [==============================] - 0s 744us/step - loss: 1.3591 - accuracy: 0.4110 - val_loss: 1.3847 - val_accuracy: 0.5833\n",
      "Epoch 4/100\n",
      "219/219 [==============================] - 0s 745us/step - loss: 1.3490 - accuracy: 0.3836 - val_loss: 1.3889 - val_accuracy: 0.4167\n",
      "Epoch 5/100\n",
      "219/219 [==============================] - 0s 755us/step - loss: 1.3379 - accuracy: 0.3927 - val_loss: 1.3926 - val_accuracy: 0.2917\n",
      "Train on 219 samples, validate on 24 samples\n",
      "Epoch 1/100\n",
      "219/219 [==============================] - 0s 740us/step - loss: 1.3826 - accuracy: 0.3059 - val_loss: 1.3720 - val_accuracy: 0.2917\n",
      "Epoch 2/100\n",
      "219/219 [==============================] - 0s 747us/step - loss: 1.3684 - accuracy: 0.3790 - val_loss: 1.3576 - val_accuracy: 0.3333\n",
      "Epoch 3/100\n",
      "219/219 [==============================] - 0s 741us/step - loss: 1.3551 - accuracy: 0.4703 - val_loss: 1.3444 - val_accuracy: 0.3333\n",
      "Epoch 4/100\n",
      "219/219 [==============================] - 0s 746us/step - loss: 1.3452 - accuracy: 0.4247 - val_loss: 1.3342 - val_accuracy: 0.3333\n",
      "Epoch 5/100\n",
      "219/219 [==============================] - 0s 741us/step - loss: 1.3358 - accuracy: 0.4338 - val_loss: 1.3239 - val_accuracy: 0.3333\n",
      "Epoch 6/100\n",
      "219/219 [==============================] - 0s 759us/step - loss: 1.3199 - accuracy: 0.4566 - val_loss: 1.3138 - val_accuracy: 0.3333\n",
      "Epoch 7/100\n",
      "219/219 [==============================] - 0s 738us/step - loss: 1.3086 - accuracy: 0.4932 - val_loss: 1.3007 - val_accuracy: 0.3333\n",
      "Epoch 8/100\n",
      "219/219 [==============================] - 0s 744us/step - loss: 1.2915 - accuracy: 0.4977 - val_loss: 1.2803 - val_accuracy: 0.3333\n",
      "Epoch 9/100\n",
      "219/219 [==============================] - 0s 744us/step - loss: 1.2646 - accuracy: 0.5114 - val_loss: 1.2485 - val_accuracy: 0.4167\n",
      "Epoch 10/100\n",
      "219/219 [==============================] - 0s 752us/step - loss: 1.2219 - accuracy: 0.5388 - val_loss: 1.1996 - val_accuracy: 0.4167\n",
      "Epoch 11/100\n",
      "219/219 [==============================] - 0s 745us/step - loss: 1.1648 - accuracy: 0.5936 - val_loss: 1.1326 - val_accuracy: 0.5417\n",
      "Epoch 12/100\n",
      "219/219 [==============================] - 0s 752us/step - loss: 1.0879 - accuracy: 0.6758 - val_loss: 1.0525 - val_accuracy: 0.6250\n",
      "Epoch 13/100\n",
      "219/219 [==============================] - 0s 751us/step - loss: 1.0048 - accuracy: 0.7489 - val_loss: 0.9578 - val_accuracy: 0.7083\n",
      "Epoch 14/100\n",
      "219/219 [==============================] - 0s 749us/step - loss: 0.9030 - accuracy: 0.7717 - val_loss: 0.8519 - val_accuracy: 0.7500\n",
      "Epoch 15/100\n",
      "219/219 [==============================] - 0s 747us/step - loss: 0.8017 - accuracy: 0.8219 - val_loss: 0.7595 - val_accuracy: 0.7917\n",
      "Epoch 16/100\n",
      "219/219 [==============================] - 0s 746us/step - loss: 0.7059 - accuracy: 0.8128 - val_loss: 0.6789 - val_accuracy: 0.7917\n",
      "Epoch 17/100\n",
      "219/219 [==============================] - 0s 752us/step - loss: 0.5982 - accuracy: 0.8721 - val_loss: 0.6019 - val_accuracy: 0.8333\n",
      "Epoch 18/100\n",
      "219/219 [==============================] - 0s 748us/step - loss: 0.4910 - accuracy: 0.8950 - val_loss: 0.4928 - val_accuracy: 0.8750\n",
      "Epoch 19/100\n",
      "219/219 [==============================] - 0s 747us/step - loss: 0.3883 - accuracy: 0.9315 - val_loss: 0.4207 - val_accuracy: 0.8750\n",
      "Epoch 20/100\n",
      "219/219 [==============================] - 0s 737us/step - loss: 0.2875 - accuracy: 0.9589 - val_loss: 0.3398 - val_accuracy: 0.8750\n",
      "Epoch 21/100\n",
      "219/219 [==============================] - 0s 739us/step - loss: 0.2125 - accuracy: 0.9726 - val_loss: 0.2918 - val_accuracy: 0.8750\n",
      "Epoch 22/100\n",
      "219/219 [==============================] - 0s 748us/step - loss: 0.1504 - accuracy: 0.9726 - val_loss: 0.2854 - val_accuracy: 0.8750\n",
      "Epoch 23/100\n",
      "219/219 [==============================] - 0s 745us/step - loss: 0.1094 - accuracy: 0.9817 - val_loss: 0.2801 - val_accuracy: 0.8750\n",
      "Epoch 24/100\n",
      "219/219 [==============================] - 0s 751us/step - loss: 0.0978 - accuracy: 0.9863 - val_loss: 0.2481 - val_accuracy: 0.8750\n",
      "Epoch 25/100\n",
      "219/219 [==============================] - 0s 750us/step - loss: 0.0689 - accuracy: 0.9909 - val_loss: 0.2256 - val_accuracy: 0.8750\n",
      "Epoch 26/100\n",
      "219/219 [==============================] - 0s 749us/step - loss: 0.0499 - accuracy: 0.9909 - val_loss: 0.2336 - val_accuracy: 0.9167\n",
      "Epoch 27/100\n",
      "219/219 [==============================] - 0s 746us/step - loss: 0.0425 - accuracy: 0.9909 - val_loss: 0.2508 - val_accuracy: 0.8333\n",
      "Epoch 28/100\n",
      "219/219 [==============================] - 0s 743us/step - loss: 0.0351 - accuracy: 0.9909 - val_loss: 0.2436 - val_accuracy: 0.8333\n",
      "Train on 220 samples, validate on 23 samples\n",
      "Epoch 1/100\n",
      "220/220 [==============================] - 0s 743us/step - loss: 1.3862 - accuracy: 0.2500 - val_loss: 1.3802 - val_accuracy: 0.4783\n",
      "Epoch 2/100\n",
      "220/220 [==============================] - 0s 740us/step - loss: 1.3789 - accuracy: 0.3636 - val_loss: 1.3709 - val_accuracy: 0.3478\n",
      "Epoch 3/100\n",
      "220/220 [==============================] - 0s 750us/step - loss: 1.3642 - accuracy: 0.3455 - val_loss: 1.3625 - val_accuracy: 0.3478\n",
      "Epoch 4/100\n",
      "220/220 [==============================] - 0s 737us/step - loss: 1.3539 - accuracy: 0.3500 - val_loss: 1.3559 - val_accuracy: 0.3478\n",
      "Epoch 5/100\n",
      "220/220 [==============================] - 0s 731us/step - loss: 1.3442 - accuracy: 0.3636 - val_loss: 1.3525 - val_accuracy: 0.3478\n",
      "Epoch 6/100\n",
      "220/220 [==============================] - 0s 735us/step - loss: 1.3336 - accuracy: 0.3864 - val_loss: 1.3489 - val_accuracy: 0.3478\n",
      "Epoch 7/100\n",
      "220/220 [==============================] - 0s 743us/step - loss: 1.3267 - accuracy: 0.4045 - val_loss: 1.3456 - val_accuracy: 0.3478\n",
      "Epoch 8/100\n",
      "220/220 [==============================] - 0s 738us/step - loss: 1.3119 - accuracy: 0.4273 - val_loss: 1.3391 - val_accuracy: 0.3478\n",
      "Epoch 9/100\n",
      "220/220 [==============================] - 0s 733us/step - loss: 1.2954 - accuracy: 0.4364 - val_loss: 1.3236 - val_accuracy: 0.3913\n",
      "Epoch 10/100\n",
      "220/220 [==============================] - 0s 738us/step - loss: 1.2630 - accuracy: 0.4773 - val_loss: 1.2974 - val_accuracy: 0.4348\n",
      "Epoch 11/100\n",
      "220/220 [==============================] - 0s 746us/step - loss: 1.2197 - accuracy: 0.5455 - val_loss: 1.2620 - val_accuracy: 0.4783\n",
      "Epoch 12/100\n",
      "220/220 [==============================] - 0s 751us/step - loss: 1.1602 - accuracy: 0.6318 - val_loss: 1.2114 - val_accuracy: 0.5217\n",
      "Epoch 13/100\n",
      "220/220 [==============================] - 0s 741us/step - loss: 1.0759 - accuracy: 0.7136 - val_loss: 1.1489 - val_accuracy: 0.5217\n",
      "Epoch 14/100\n",
      "220/220 [==============================] - 0s 742us/step - loss: 0.9787 - accuracy: 0.7545 - val_loss: 1.0751 - val_accuracy: 0.6522\n",
      "Epoch 15/100\n",
      "220/220 [==============================] - 0s 744us/step - loss: 0.8682 - accuracy: 0.7864 - val_loss: 0.9972 - val_accuracy: 0.6522\n",
      "Epoch 16/100\n",
      "220/220 [==============================] - 0s 741us/step - loss: 0.7680 - accuracy: 0.8091 - val_loss: 0.9256 - val_accuracy: 0.6957\n",
      "Epoch 17/100\n",
      "220/220 [==============================] - 0s 731us/step - loss: 0.6640 - accuracy: 0.8364 - val_loss: 0.8566 - val_accuracy: 0.6957\n",
      "Epoch 18/100\n",
      "220/220 [==============================] - 0s 741us/step - loss: 0.5382 - accuracy: 0.8864 - val_loss: 0.7993 - val_accuracy: 0.6522\n",
      "Epoch 19/100\n",
      "220/220 [==============================] - 0s 751us/step - loss: 0.4047 - accuracy: 0.9318 - val_loss: 0.7109 - val_accuracy: 0.7391\n",
      "Epoch 20/100\n",
      "220/220 [==============================] - 0s 743us/step - loss: 0.3095 - accuracy: 0.9318 - val_loss: 0.6131 - val_accuracy: 0.7826\n",
      "Epoch 21/100\n",
      "220/220 [==============================] - 0s 741us/step - loss: 0.2354 - accuracy: 0.9500 - val_loss: 0.6874 - val_accuracy: 0.6957\n",
      "Epoch 22/100\n",
      "220/220 [==============================] - 0s 743us/step - loss: 0.1763 - accuracy: 0.9636 - val_loss: 0.6208 - val_accuracy: 0.6957\n",
      "Epoch 23/100\n",
      "220/220 [==============================] - 0s 738us/step - loss: 0.1329 - accuracy: 0.9727 - val_loss: 0.5576 - val_accuracy: 0.7826\n",
      "Epoch 24/100\n",
      "220/220 [==============================] - 0s 737us/step - loss: 0.0959 - accuracy: 0.9818 - val_loss: 0.5121 - val_accuracy: 0.8696\n",
      "Epoch 25/100\n",
      "220/220 [==============================] - 0s 744us/step - loss: 0.0752 - accuracy: 0.9864 - val_loss: 0.4968 - val_accuracy: 0.8696\n",
      "Epoch 26/100\n",
      "220/220 [==============================] - 0s 747us/step - loss: 0.0600 - accuracy: 0.9864 - val_loss: 0.5368 - val_accuracy: 0.8696\n",
      "Epoch 27/100\n",
      "220/220 [==============================] - 0s 733us/step - loss: 0.0446 - accuracy: 0.9909 - val_loss: 0.5818 - val_accuracy: 0.8696\n",
      "Epoch 28/100\n",
      "220/220 [==============================] - 0s 742us/step - loss: 0.0352 - accuracy: 0.9909 - val_loss: 0.6077 - val_accuracy: 0.8696\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>fold_idx</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RNN</td>\n",
       "      <td>0</td>\n",
       "      <td>0.760000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RNN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.880000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RNN</td>\n",
       "      <td>2</td>\n",
       "      <td>0.840000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RNN</td>\n",
       "      <td>3</td>\n",
       "      <td>0.880000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RNN</td>\n",
       "      <td>4</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>RNN</td>\n",
       "      <td>5</td>\n",
       "      <td>0.875000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>RNN</td>\n",
       "      <td>6</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>RNN</td>\n",
       "      <td>7</td>\n",
       "      <td>0.791667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>RNN</td>\n",
       "      <td>8</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>RNN</td>\n",
       "      <td>9</td>\n",
       "      <td>0.826087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>RNN with Attention</td>\n",
       "      <td>0</td>\n",
       "      <td>0.760000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>RNN with Attention</td>\n",
       "      <td>1</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>RNN with Attention</td>\n",
       "      <td>2</td>\n",
       "      <td>0.880000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>RNN with Attention</td>\n",
       "      <td>3</td>\n",
       "      <td>0.840000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>RNN with Attention</td>\n",
       "      <td>4</td>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>RNN with Attention</td>\n",
       "      <td>5</td>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>RNN with Attention</td>\n",
       "      <td>6</td>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>RNN with Attention</td>\n",
       "      <td>7</td>\n",
       "      <td>0.291667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>RNN with Attention</td>\n",
       "      <td>8</td>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>RNN with Attention</td>\n",
       "      <td>9</td>\n",
       "      <td>0.869565</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            model_name  fold_idx  accuracy\n",
       "0                  RNN         0  0.760000\n",
       "1                  RNN         1  0.880000\n",
       "2                  RNN         2  0.840000\n",
       "3                  RNN         3  0.880000\n",
       "4                  RNN         4  0.666667\n",
       "5                  RNN         5  0.875000\n",
       "6                  RNN         6  0.750000\n",
       "7                  RNN         7  0.791667\n",
       "8                  RNN         8  0.916667\n",
       "9                  RNN         9  0.826087\n",
       "10  RNN with Attention         0  0.760000\n",
       "11  RNN with Attention         1  0.800000\n",
       "12  RNN with Attention         2  0.880000\n",
       "13  RNN with Attention         3  0.840000\n",
       "14  RNN with Attention         4  0.833333\n",
       "15  RNN with Attention         5  0.833333\n",
       "16  RNN with Attention         6  0.833333\n",
       "17  RNN with Attention         7  0.291667\n",
       "18  RNN with Attention         8  0.833333\n",
       "19  RNN with Attention         9  0.869565"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "model_name\n",
       "RNN                   0.818609\n",
       "RNN with Attention    0.777457\n",
       "Name: accuracy, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 936x595.44 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def KerasKFoldCrossVal(models,X,Y,fold=10):\n",
    "    from sklearn.model_selection import StratifiedKFold\n",
    "    seed = np.random.randint(low=1, high=100)\n",
    "    #np.random.seed(seed)\n",
    "    figureSize = (13,8.27)\n",
    "    sns.set(rc={'figure.figsize':figureSize})\n",
    "    entries = []\n",
    "    cv_df = pd.DataFrame(index=range(fold * len(models))) \n",
    "    kfold = StratifiedKFold(n_splits=fold, shuffle=True, random_state=seed)\n",
    "    model_names = ['RNN','RNN with Attention']\n",
    "    for i,model in enumerate(models):\n",
    "        model_name = model_names[i]\n",
    "        for fold_idx,dataSlice in enumerate(kfold.split(X, Y)): \n",
    "            \n",
    "            train = dataSlice[0]\n",
    "            test = dataSlice[1]\n",
    "\n",
    "            tempX = tokenizer.texts_to_sequences(X[train])\n",
    "            tempX = pad_sequences(tempX, maxlen=MAX_LENGTH)\n",
    "            tempY = pd.get_dummies(Y[train])\n",
    "            testX = tokenizer.texts_to_sequences(X[test])\n",
    "            testX = pad_sequences(testX, maxlen=MAX_LENGTH)\n",
    "            testY = pd.get_dummies(Y[test])\n",
    "\n",
    "            model.load_weights('{} Initializer.h5'.format(model_name))\n",
    "            \n",
    "            epochs = 100\n",
    "            batch_size = 64\n",
    "            history = model.fit(tempX,tempY, epochs=epochs, batch_size=batch_size ,validation_data=(testX,testY), callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)])\n",
    "            # evaluate the model\n",
    "            accuracy = history.history['val_accuracy']\n",
    "            accuracy = accuracy[-1]\n",
    "            entries.append([model_name,fold_idx, accuracy])\n",
    "    cv_df = pd.DataFrame(entries, columns=['model_name', 'fold_idx', 'accuracy']) \n",
    "    display(cv_df)\n",
    "    sns.boxplot(x='model_name', y='accuracy', data=cv_df) \n",
    "    sns.stripplot(x='model_name', y='accuracy', data=cv_df, size=8, jitter=True, edgecolor=\"gray\", linewidth=2)\n",
    "    KerasKFoldMean = cv_df.groupby('model_name').accuracy.mean().sort_values(ascending=False)\n",
    "    return KerasKFoldMean\n",
    "\n",
    "\n",
    "models = [RNN,RNNwithAttention]\n",
    "doc = pd.read_excel('data.xlsx',header=0)\n",
    "doc = doc.sample(frac=1).reset_index(drop=True)\n",
    "doc = data_preprocessing(doc)\n",
    "\n",
    "pureX = doc[\"Question\"].values\n",
    "pureY = doc[\"Label\"].values\n",
    "KfoldMean = KerasKFoldCrossVal(models, pureX, pureY)\n",
    "KfoldMean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00472257 0.00362998 0.00855551 0.983092  ]] The Master Theorem\n"
     ]
    }
   ],
   "source": [
    "new_question = ['Recursion tree diagram to find the solution to the recurrence. Assume n is a power of 2.']\n",
    "seq = tokenizer.texts_to_sequences(new_question)\n",
    "padded = pad_sequences(seq, maxlen=MAX_LENGTH)\n",
    "pred = model.predict(padded)\n",
    "labels = [\"Counting Lists, Permutations, and Subsets\",'Details of the RSA Cryptosystem', 'Propositional logic, Predicate logic, Inference and proofs', 'The Master Theorem']\n",
    "print(pred, labels[np.argmax(pred)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
